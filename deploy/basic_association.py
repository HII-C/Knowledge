#!/usr/bin/env python3


import contextlib as __stickytape_contextlib

@__stickytape_contextlib.contextmanager
def __stickytape_temporary_dir():
    import tempfile
    import shutil
    dir_path = tempfile.mkdtemp()
    try:
        yield dir_path
    finally:
        shutil.rmtree(dir_path)

with __stickytape_temporary_dir() as __stickytape_working_dir:
    def __stickytape_write_module(path, contents):
        import os, os.path

        def make_package(path):
            parts = path.split("/")
            partial_path = __stickytape_working_dir
            for part in parts:
                partial_path = os.path.join(partial_path, part)
                if not os.path.exists(partial_path):
                    os.mkdir(partial_path)
                    open(os.path.join(partial_path, "__init__.py"), "w").write("\n")

        make_package(os.path.dirname(path))

        full_path = os.path.join(__stickytape_working_dir, path)
        with open(full_path, "w") as module_file:
            module_file.write(contents)

    import sys as __stickytape_sys
    __stickytape_sys.path.insert(0, __stickytape_working_dir)

    __stickytape_write_module('knowledge/model/association/basic/model.py', '\nimport gzip\nimport pickle\nimport logging as log\n\nfrom enum import IntEnum\nfrom typing import Dict\n\nfrom knowledge.struct.association import Association\nfrom knowledge.struct.fpgrowth import Fpgrowth\nfrom knowledge.struct.transaction import Transaction\nfrom knowledge.struct.dimension import Dimension\nfrom knowledge.util.config import ConfigUtil\nfrom knowledge.util.filesys import FilesysUtil\n\n\ndef mulitopen(filepath, **kwargs):\n    \'autodetect compressed file\'\n\n    if filepath.split(\'.\')[-1] == \'gz\':\n        data = gzip.open(filepath, **kwargs)\n    else:\n        data = open(filepath, **kwargs)\n    return data\n\n\nclass Activity(IntEnum):\n    TRANSACTIONS = 0\n    TREE = 1\n    PATTERNS = 2\n    ASSOCIATIONS = 3\n    \n    @classmethod\n    def decode(self, string):\n        return getattr(self, string.upper())\n\n\nclass BasicAssociationModel:\n    def __init__(self, config):\n        self.config = config\n\n\n    @classmethod\n    def configure(self, config_file, specs_file, validate=True):\n        config = ConfigUtil.load_config(config_file)\n        specs = ConfigUtil.load_specs(specs_file)\n\n        if validate:\n            config = self.validate(config)\n\n        return ConfigUtil.verify_config(specs, config)\n\n\n    @classmethod\n    def validate(self, config):\n        warn = 0\n        fail = 0\n\n        source = Activity.decode(config[\'source\'])\n        goal = Activity.decode(config[\'goal\'])\n        load = config[source.name.lower()][\'file\']\n        save = config[goal.name.lower()][\'file\']\n\n        if source < goal:\n            fail += 1\n            log.error(f\'Does not make sense to try to find/make {goal.name.lower()} \'\n                f\'from {source.name.lower()}; consider swapping goal and source.\')\n        elif source == goal:\n            warn += 1\n            log.error(f\'Does not make sense to have source and goal be \'\n                \'the same; nothing will be done.\')\n\n        if load is None:\n            fail += 1\n            log.error(f\'Property "file" needs to defined on source concpet (\'\n                f\'{source.name.lower()}) so data can be loaded.\')\n        if save is None:\n            fail += 1\n            log.error(f\'Property "file" needs to defined on goal concpet (\'\n                f\'{source.name.lower()}) so data can be saved.\')\n        \n        for act in Activity:\n            if act > source and act < goal:\n                if config[act][\'save\']:\n                    if FilesysUtil.file_exists(config[act][\'file\']):\n                        warn += 1\n                        log.info(f\'File conflict path: {config[act]["file"]}.\')\n                        log.warning(f\'The file specified to save {act.name.lower()}\'\n                            \'already exists; it will be overwritten if model runs.\')\n\n        # TODO: more error handling\n\n        if fail:\n            log.error(f\'{fail} critical and {warn} potential issues found in model \'\n                \'configuration (see above errors). Check config file and rerun.\')\n            raise ValueError\n\n        if warn:\n            log.warning(f\'{warn} potential issues found in model configuration \'\n                \'(see above warnings). Would you like to continue model run? [Y/n]\')\n            if not input().lower() in (1, \'y\', \'yes\', \'yeet\'):\n                log.error(\'User chose to terminate process.\')\n                raise RuntimeError\n\n        return config\n\n\n    def run(self):\n        source = Activity.decode(self.config[\'source\'])\n        goal = Activity.decode(self.config[\'goal\'])\n\n        support: Dict[str, int] = None\n        fpgrowth: Fpgrowth = None\n        patterns: Dict[frozenset, int] = None\n        association: Association = None\n\n        if source == Activity.TRANSACTIONS:\n            filepath = self.config[\'transactions\'][\'file\']\n            support = Fpgrowth.calculate_support(filepath)\n            fpgrowth = Fpgrowth(support)\n            fpgrowth.load_transactions(filepath)\n        \n        if source == Activity.TREE:\n            filepath = self.config[\'tree\'][\'file\']\n            data = mulitopen(filepath, mode=\'rb\')\n            fpgrowth = pickle.load(data)\n            data.close()\n\n        if source < Activity.TREE and goal >= Activity.TREE:\n            if self.config[\'tree\'][\'save\']:\n                filepath = self.config[\'tree\'][\'file\']\n                data = mulitopen(filepath, mode=\'wb\')\n                pickle.dump(fpgrowth, data)\n                data.close()\n\n        if source == Activity.PATTERNS:\n            filepath = self.config[\'patterns\'][\'file\']\n            patterns = Fpgrowth.load_patterns(filepath)\n\n        if source < Activity.PATTERNS and goal >= Activity.PATTERNS:\n            patterns = fpgrowth.find_patterns(\n                min_support=self.config[\'patterns\'][\'min_support\'],\n                max_support=self.config[\'patterns\'][\'max_support\'],\n                max_size=self.config[\'patterns\'][\'max_size\'],\n                cores=self.config[\'run\'][\'cores\'])\n            if self.config[\'patterns\'][\'save\']:\n                filepath = self.config[\'patterns\'][\'file\']\n                Fpgrowth.write_patterns(patterns, filepath)\n\n        if goal == Activity.ASSOCIATIONS:\n            filepath = self.config[\'associations\'][\'file\']\n            association = Association(patterns)\n            association.find_associations(filepath)\n            \n\n        \n\n                \n\n\n\n')
    __stickytape_write_module('knowledge/struct/association.py', '\nimport pickle\nimport gzip\nimport csv\nimport logging as log\n\nfrom itertools import combinations\nfrom multiprocessing import Pool, Manager, Value\nfrom ctypes import c_uint64\n\nfrom knowledge.struct.fpgrowth import Fpgrowth\nfrom knowledge.util.filesys import FilesysUtil\n\n\ncount = None\nn = None\n\ndef mulitopen(filepath, **kwargs):\n    \'autodetect compressed file\'\n\n    if filepath.split(\'.\')[-1] == \'gz\':\n        data = gzip.open(filepath, **kwargs)\n    else:\n        data = open(filepath, **kwargs)\n    return data\n\n\ndef chunks(lst, n):\n    for i in range(0, len(lst), n):\n        yield lst[i:i + n]\n\n\ndef write_associations(queue, csv, cols):\n    \'\'\'asynchronous queued association csv write\n\n    Parameters\n    ----------\n    queue: multiprocessing.Queue\n        File write queue as provided by the multiprocessing manager.\n\n    csv: str\n        String for the filepath to write associations.\n\n    cols: list[str]\n        List of column names for csv output.    \'\'\'\n    \n    csvfile = mulitopen(csv, mode=\'w\', newline=\'\')\n    csvwriter = csv.writer(csvfile, delimiter=\',\', quotechar=\'"\')\n    csvwriter.writerow(cols)\n\n    request = queue.get()\n    while request != \'kill\':\n        csvwriter.writerows(request)\n        csvfile.flush()\n        request = queue.get()\n    csvfile.close()\n\n\ndef find_associations(queue, data, keys, min_support, min_confidence):\n    \'\'\'find associations in pattern chucnk and add to write queue\n\n    Parameters\n    ----------\n    queue: multiprocessing.Queue\n        File write queue as provided by the multiprocessing manager.\n\n    patterns: dict{frozenset[str]: float}\n        Dictionary of frequent patterns where a frozenset of items map\n        to their corresponding support.\n\n    keys: list[frozenset[str]]\n        List of frozensets that are keys to the patterns dictionary;\n        these patterns will be analyzed for association finding.\n\n    min_support: float\n\n    min_confidence: float   \'\'\'\n\n    patterns = data()\n    inf = float(\'inf\')\n    metrics = {\n        \'support\':    lambda sAC, sA, sC: sAC,\n        \'confidence\': lambda sAC, sA, sC: sAC/sA,\n        \'lift\':       lambda sAC, sA, sC: sAC/sA/sC,\n        \'leverage\':   lambda sAC, sA, sC: sAC-sA*sC,\n        \'rpf\':        lambda sAC, sA, sC: sAC*sAC/sA,\n        \'conviction\': lambda sAC, sA, sC: (1-sC)/(1-sAC/sA) \\\n            if sAC != sA else inf   }\n    \n    associations = []\n    local_count = 0\n    for pattern in keys:\n        sAC = patterns[pattern]\n        for idx in range(len(pattern)-1,0,-1):\n            for subset in combinations(pattern, r=idx):\n                antecedent = frozenset(subset)\n                consequent = pattern - antecedent\n\n                sA = patterns[antecedent]\n                sC = patterns[consequent]\n\n                score = (metrics[\'support\'](sAC, sA, sC) >= min_support and\n                    metrics[\'confidence\'](sAC, sA, sC) >= min_confidence)\n\n                if score:\n                    if count.value >= n.value:\n                        log.info(f\'Found association {count.value}.\')\n                        with n.get_lock():\n                            n.value <<= 1\n                    \n                    if local_count >= 100000:\n                        queue.put(associations)\n                        associations = []\n                        local_count = 0 \n                        \n                    associations.append((\n                        \',\'.join(sorted(antecedent)),\n                        \',\'.join(sorted(consequent)),\n                        metrics[\'support\'](sAC, sA, sC),\n                        metrics[\'confidence\'](sAC, sA, sC),\n                        metrics[\'lift\'](sAC, sA, sC),\n                        metrics[\'leverage\'](sAC, sA, sC),\n                        metrics[\'conviction\'](sAC, sA, sC) if sAC != sA else None,\n                        metrics[\'rpf\'](sAC, sA, sC)))\n\n                    local_count += 1\n                    with count.get_lock():\n                        count.value += 1\n\n    queue.put(associations)\n\n\nclass Association:\n    def __init__(self, patterns):\n        self.patterns = patterns\n\n\n    def get_association(self, consequent, antecedent):\n        \'get a specific association from patterns\'\n        \n        pass\n\n\n    def find_associations(self, savepath, min_support=0, min_confidence=0, \n            cores=None):\n        \'find associations from frequent patterns dictionary\'\n\n        log.info(f\'Balancing patterns into tasks for {cores} cores.\')\n\n        global count, n\n        count = Value(c_uint64)\n        count.value = 0\n        n = Value(c_uint64)\n        n.value = 1\n\n        manager = Manager()\n        queue = manager.Queue(maxsize=10)\n        pool = Pool(processes=cores)\n\n        cols = (\'antecedent\', \'consequent\', \'support\', \'confidence\', \'lift\',\n            \'leverage\', \'conviction\', \'rpf\')\n        pool.apply_async(write_associations, (queue, savepath, cols))\n\n        chunksize = len(self.patterns) // (cores * 4)\n        data = lambda: self.patterns\n        jobs = chunks(list(self.patterns.keys()), chunksize)\n        tasks = ((queue, data, keys, min_support, min_confidence) for keys in jobs)\n\n        log.info(f\'Finding associations and writing them to {savepath}.\')\n        pool.starmap(find_associations, tasks)\n        queue.put(\'kill\')\n        pool.close()\n        pool.join()\n\n        if count.value != n.value >> 1:\n            log.info(f\'Found association {count.value}.\')\n        ')
    __stickytape_write_module('knowledge/struct/fpgrowth.py', '\nimport gzip\nimport csv\nimport pandas as pd\nimport numpy as np\nimport logging as log\n\nfrom collections import defaultdict\nfrom itertools import combinations\nfrom multiprocessing import Pool, Value\nfrom ctypes import c_uint64\n\n\ncount = None\nn = None\n\n\ndef mulitopen(filepath, **kwargs):\n    \'autodetect compressed file\'\n\n    if filepath.split(\'.\')[-1] == \'gz\':\n        data = gzip.open(filepath, **kwargs)\n    else:\n        data = open(filepath, **kwargs)\n    return data\n\n\ndef find_patterns(tree, min_support, max_support, max_size):\n    \'pattern finding function for a single thread\'\n\n    generator = Fpgrowth.generate_patterns(tree, min_support, \n        max_support, max_size)\n    patterns = []\n    for pattern in generator:\n        patterns.append(pattern)\n        if count.value >= n.value:\n            with n.get_lock():\n                n.value <<= 1\n            log.info(f\'Found pattern {count.value}.\')\n        with count.get_lock():\n            count.value += 1\n    return patterns\n\n\nclass Fpgrowth:\n    \'\'\'data structure and utilities for running fpgrowth algorithm\n\n    Parameters\n    ----------\n    support: dict{str: int}\n        A dictionary mapping the names of items to their support;\n        not the support here is an integer between zero and the \n        number of transactions, not a float between 0 ad 1. \'\'\'\n\n    def __init__(self, support):\n        self.tree = Tree()\n        self.support = support\n\n\n    @classmethod\n    def generate_patterns(self, tree, min_support=0, max_support=1, max_size=0):\n        \'\'\'recursively generates frequent patterns off of tree\n        \n        Parameters\n        ----------\n        tree: Tree\n            The tree to read patterns from; this function will continue to\n            recursively itself with subtrees until the tree is a path or empty.\n\n        min_support: int\n            The minimum support of a pattern for it to be included in the\n            list of frequent patterns.\n\n        max_support: int\n\n        max_size: int\n\n\n        Yields\n        ------\n        pattern: tuple(float, list[str])    \'\'\'\n\n        items = tree.nodes.keys()\n        if tree.is_path():\n            size_remain = len(items) + 1\n            if max_size:\n                size_remain = max_size - len(tree.items) + 1\n            for i in range(1, size_remain):\n                for itemset in combinations(items, i):\n                    support = min([tree.nodes[i][0].count for i in itemset])\n                    yield support, tree.items + list(itemset)\n        elif max_size == 0  or max_size > len(tree.items):            \n            for item in items:\n                support = sum([node.count for node in tree.nodes[item]])\n                yield support, tree.items + [item]\n                \n            for item in items:\n                subtree = tree.conditional_tree(item, min_support, max_support)\n                for support, itemset in self.generate_patterns(subtree, \n                        min_support, max_support, max_size):\n                    yield support, itemset\n\n        \n    @classmethod\n    def calculate_support(self, filepath):\n        csvfile = mulitopen(filepath, mode=\'r\')\n        csvreader = csv.reader(csvfile, delimiter=\',\', quotechar=\'"\')\n        count = 0\n        n = 1\n\n        _, *items = next(csvreader)\n        support = [0]*len(items)\n        for transaction in csvreader:\n            for item, cond in enumerate(transaction):\n                if cond:\n                    support[item] += 1\n                count += 1\n                if count == n:\n                    log.info(f\'Processed code {count}.\')\n                    n <<= 1\n        support = dict(zip(items, support))\n\n        # alterate file format\n\n        # support = defaultdict(int)\n        # for transaction in csvreader:\n        #     for code in transaction:\n        #         support[code] += 1\n        #         count += 1\n        #         if count == n:\n        #             log.info(f\'Processed code {count}.\')\n        #             n <<= 1\n\n        if count != n >> 1:\n            log.info(f\'Processed code {count}.\')\n        csvfile.close()\n\n        return support\n\n    \n    @classmethod\n    def load_patterns(self, filepath):\n        csvfile = mulitopen(filepath, mode=\'r\')\n        csvwriter = csv.reader(csvfile, delimiter=\',\', quotechar=\'"\')\n        patterns = {}\n\n        for row in csvwriter:\n            patterns[frozenset(row[:-1])] = int(row[-1])\n\n        csvfile.close()\n        return patterns\n\n\n    @classmethod\n    def write_patterns(self, patterns, filepath):\n        csvfile = mulitopen(filepath, mode=\'w\')\n        csvwriter = csv.writer(csvfile, delimiter=\',\', quotechar=\'"\')\n\n        for codes, support in patterns.items():\n            csvwriter.writerow((*codes, support))\n\n        csvfile.close()\n\n\n    def load_transactions(self, filepath):\n        csvfile = mulitopen(filepath, mode=\'r\')\n        csvreader = csv.reader(csvfile, delimiter=\',\', quotechar=\'"\')\n        count = 0\n        n = 1\n        \n        _, *items = next(csvreader)\n        for transaction in csvreader:\n            itemset = [item for cond, item in zip(transaction[1:], items) \n                if cond and item in self.support]\n            itemset.sort(key=self.support.get, reverse=True)\n            self.tree.insert_itemset(itemset)\n            count += 1\n            if count == n:\n                log.info(f\'Processed transaction {count}.\')\n                n <<= 1\n\n        # alternate file format\n\n        # for transaction in csvreader:\n        #     self.tree.insert_itemset(transaction)\n        #     count += 1\n        #     if count == n:\n        #         log.info(f\'Processed transaction {count}.\')\n        #         n <<= 1\n\n        if count != n >> 1:\n            log.info(f\'Processed transaction {count}.\')\n        csvfile.close()\n\n\n    def find_patterns(self, tree=None, min_support=0, max_support=1, \n            max_size=0, cores=None):\n        \'\'\'finds patterns from tree using multiprocessing\n\n        Parameters\n        ----------\n        tree: Tree\n            The tree to read patterns from\n\n        min_support: int\n            The minimum support of a pattern for it to be included in the\n            list of frequent patterns.\n\n        max_support: int\n            \n        max_size: int\n\n        cores: int/None\n            Number of cores to utilize; default is None, which will auto detect\n            the number of cores available to use.\n\n        Returns\n        -------\n        patterns: list[list[float, list[str]]]\n            A list of patterns, where each pattern is a list containing\n            the pattern support value and list of items in the pattern. \'\'\'\n        \n        tree = self.tree if tree is None else tree\n        items = tree.nodes.keys()\n        subtrees = []\n        patterns = []\n\n        log.info(f\'Balancing tree into tasks for {cores} cores.\')\n\n        for item in items:\n            subtree = tree.conditional_tree(item, min_support, max_support)\n            subtrees.append((subtree, min_support, max_support, max_size))\n        subtrees.sort(key=lambda tree: tree[0].root.count_descendents(), reverse=True)\n        \n        global count, n\n        count = Value(c_uint64)\n        count.value = 0\n        n = Value(c_uint64)\n        n.value = 1\n\n        log.info(f\'Finding patterns from root node.\')\n\n        for item in items:\n            support = sum([node.count for node in tree.nodes[item]])\n            patterns.append((support, (item,)))\n            if count.value >= n.value:\n                log.info(f\'Found pattern {count.value}.\')\n                n.value <<= 1\n            count.value += 1\n\n        log.info(f\'Now finding remaining patterns on {cores} cores.\')\n\n        pool = Pool(processes=cores)\n        for result in pool.starmap(find_patterns, subtrees):\n            patterns.extend(result)\n        if count.value != n.value >> 1:\n            log.info(f\'Found pattern {count.value}.\')\n        pool.close()\n        pool.join()\n        \n        return patterns\n\n\nclass Tree:\n    \'\'\'a tree structure with variable length, unordered children\n\n    Parameters\n    ----------\n    root: str/int/float = None\n        The value of the root node; default is None.    \'\'\'\n\n    def __init__(self, root=None):\n        self.root = Node(root)\n        self.nodes = defaultdict(list)\n        self.items = []\n    \n\n    def insert_itemset(self, itemset, count=1):\n        node = self.root\n        node.count += count\n\n        idx = 0\n        for item in itemset:\n            if item in node.children:\n                node = node.children[item]\n                node.count += count\n                idx += 1\n            else:\n                break\n\n        for item in itemset[idx:]:\n            child = Node(item, count=count, parent=node)\n            node.children[item] = child\n            self.nodes[item].append(child)\n            node = child\n            \n\n    def conditional_tree(self, cond, min_support, max_support):\n        branches = []\n        count = defaultdict(int)\n        for node in self.nodes[cond]:\n            branch = node.itempath()\n            branches.append(branch)\n            for item in branch:\n                count[item] += node.count\n                \n        items = [item for item in count if count[item] >= min_support\n            and count[item] <= max_support]\n        items.sort(key=count.get)\n        rank = {item: i for i, item in enumerate(items)}\n\n        tree = Tree()\n        tree.items = self.items + [cond]\n        for idx, branch in enumerate(branches):\n            branch = sorted([node for node in branch if node in rank],\n                key=rank.get, reverse=True)\n            tree.insert_itemset(branch, self.nodes[cond][idx].count)\n\n        return tree\n\n\n    def is_path(self):\n        node = self.root\n        while len(node.children) == 1:\n            node = list(node.children.values())[0]\n        return len(node.children) == 0\n\n\nclass Node:\n    \'\'\'an element in a tree structure\n\n    Parameters\n    ----------\n    item: str/int/float\n        A value describing the name of the item the node is associated with.\n    \n    count: int = 1\n        What value to initialize the count of the node on; default is 1.\n\n    parent: Node = None\n        The parent node of the node being constructed; default is None type.    \'\'\'\n\n    def __init__(self, item, count=1, parent=None):\n        self.item = item\n        self.count = count\n        self.children = {}\n        self.parent = parent\n\n\n    def count_descendents(self):\n        return sum(node.count_descendents() for node in self.children.values()) + 1\n\n\n    def itempath(self):\n        path = []\n        parent = self.parent\n        while parent.item is not None:\n            path.append(parent.item)\n            parent = parent.parent\n        path.reverse()\n        return path\n')
    __stickytape_write_module('knowledge/__init__.py', '')
    __stickytape_write_module('knowledge/struct/__init__.py', '')
    __stickytape_write_module('knowledge/util/filesys.py', "\nimport os\nimport sys\nimport tempfile\nimport subprocess\nimport logging as log\n\nfrom knowledge.util.print import PrintUtil\n\nclass FilesysUtil:\n    @classmethod\n    def file_readable(self, filepath):\n        'check that file can be read'\n        return os.access(filepath, os.R_OK)\n\n\n    @classmethod\n    def file_exists(self, filepath):\n        'check that file exists'\n        return os.path.exists(filepath)\n    \n\n    @classmethod\n    def file_writable(self, filepath):\n        'check that file can be written to'\n        if self.file_exists(filepath):\n            if os.path.isfile(filepath):\n                return os.access(filepath, os.W_OK)\n            else:\n                return False \n        \n        pdir = os.path.dirname(filepath)\n        if not pdir: \n            pdir = '.'\n        return os.access(pdir, os.W_OK)\n\n\n    @classmethod\n    def create_tempfile(self, suffix=None, delete=True):\n        'creates a temporary file'\n        return tempfile.NamedTemporaryFile(suffix=suffix, delete=delete)\n\n    \n    @classmethod\n    def delete_file(self, filepath):\n        'deletes a file'\n        os.remove(filepath)\n\n\n    @classmethod\n    def format_xml(self, source, target=None):\n        'format an xml file'\n        if target is None:\n            targetfile = self.create_tempfile(suffix='xml', delete=False)\n            target = targetfile.name\n            targetfile.close()\n            result = subprocess.run(f'xmllint --format {source} > {target}', shell=True)\n            if result:\n                log.error(f'Failed to format XML on {target}.')\n            else:\n                subprocess.run(('mv', target, source), shell=False)\n        else:\n            result = subprocess.run(f'xmllint --format {source} > {target}', shell=True)\n            if result:\n                log.error(f'Failed to format XML from {source} to {target}.')\n        \n        return not bool(result)\n\n\n    @classmethod\n    def decompress(self, source, target=None, keep=False):\n        'decompress a gz file'\n        keep = '--keep' if keep else ''\n        if target is None:\n            targetfile = self.create_tempfile(\n                suffix=source.split('.')[-2], delete=False)\n            target = targetfile.name\n            targetfile.close()\n        subprocess.run(f'gunzip --stdout {keep} {source} > {target}', shell=True)\n\n        return target\n\n\n    @classmethod\n    def compress(self, source, target=None, keep=False):\n        pass\n\n\n    @classmethod\n    def package_resource(self, package, resource):\n        'get filepath of a package file'\n        try:\n            module = os.path.dirname(sys.modules[package].__file__)\n            return os.path.join(module, resource)\n        except:\n            return None")
    __stickytape_write_module('knowledge/util/print.py', "\nimport math\nimport os\nimport re\nimport time\nimport subprocess\n\nfrom datetime import datetime\nfrom getpass import getpass\n\nclass PrintUtil:\n    persist_str = ''\n    persist_rows = 0\n    logfile = None\n    silent = False\n    FRMTS = {\n        'bold': 1,\n        'faint': 2,\n        'italic': 3,\n        'underline': 4,\n        'strikethrough': 9\n    }\n\n    @classmethod\n    def silence(self):\n        self.silent = True\n\n    @classmethod\n    def unsilence(self):\n        self.silence = False\n\n    @classmethod\n    def log(self, filename):\n        self.logfile = open(filename, 'w')\n\n    @classmethod\n    def render_width(self, string):\n        # https://en.wikipedia.org/wiki/ANSI_escape_code\n        return len(re.sub('\\\\x1b\\[[0-9]*m', '', string))\n\n    @classmethod\n    def render_rows(self, string):\n        _, cols = map(int, subprocess.run(('stty', 'size'), \n            stdout=subprocess.PIPE).stdout.decode().replace('\\n', '').split())\n        return sum(self.render_width(line) // cols + 1 for line \n            in string.split('\\n'))\n\n    @classmethod\n    def table(self, tbl, align='l', pad=1, border=False, wrap=False, hrule=None):\n        if align in ('r', 'l'):\n            aligns = [align] * len(tbl[0])\n        elif type(align) in (list, tuple) and all([a in ('r', 'l') for a in align]):\n            aligns = align \n        else:\n            return ''\n        if type(pad) is int:\n            pads = [pad] * len(tbl[0])\n        elif type(pad) in (list, tuple) and all([type(p) is int for p in pad]):\n            pads = pad\n        else:\n            return ''\n        tbl = [[str(cell) for cell in row] for row in tbl]\n        widths = [max([self.render_width(cell) for cell in col]) \n            for col in list(map(list, zip(*tbl)))]\n        if border:\n            if hrule is None:\n                hrules = [0]*(len(tbl))\n            elif (type(hrule) in (list, tuple) \n                    and all([type(h) is int for h in hrules])):\n                hrules = [1 if i in hrule else 0 for i in range(len(tbl)-1)] + [0]\n            top = '+' + '+'.join('-'*(w+p*2) for w, p in zip(widths, pads)) + '+'\n            return (top + '\\n' +\n                '\\n'.join('|' + '|'.join([' '*p + cell.ljust(w) + ' '*p \n                if a == 'l' else ' '*p + cell.rjust(w) + ' '*p\n                for cell, w, a, p in zip(row, widths, aligns, pads)]) + \n                (f'|\\n{top}' if hrule else '|')\n                for row, hrule in zip(tbl, hrules)) + '\\n' + top)\n        else:\n            return '\\n'.join(''.join(cell.ljust(w) + ' '*p\n                if a == 'l' else cell.rjust(w) + ' '*p\n                for cell, w, a, p in zip(row, widths, aligns, pads)) \n                for row in tbl)\n    \n    @classmethod\n    def time(self, string):\n        date = datetime.now()\n        return ('[' + date.strftime('%H:%M:%S:') + \n            str(date.microsecond // 1000).zfill(3) +\n            '] ' + string)\n\n    @classmethod\n    def format(self, string, *frmts):\n        # https://en.wikipedia.org/wiki/ANSI_escape_code\n        codes = tuple(self.FRMTS[frmt] for frmt in frmts if frmt in self.FRMTS)\n        return ('\\x1b[%sm'*len(codes) % codes) + string + '\\x1b[0m'        \n    \n    @classmethod\n    def progress(self, string, prog):\n        prog = min(1, max(0, prog))\n        perc = 100 * prog\n        return ( string + ' [' + \n                '=' * int(perc // 5) + \n                '_' * int(20 - perc // 5) + \n                '] ' + str(round(perc, 1)) + '%')\n\n    @classmethod\n    def clear(self, rows=None):\n        if rows is None:\n            rows, cols = os.popen('stty size', 'r').read().split()\n            rows = int(rows)\n            cols = int(cols)\n        print('\\n'*(rows-1) + '\\033[F'*rows, end='\\r')\n\n    @classmethod\n    def delete(self, rows):\n        pass\n\n    @classmethod\n    def push(self):\n        persist = self.persist_str\n        self.print('', persist=True, replace=True)\n        self.print(persist)\n\n    @classmethod\n    def printer(self, *args, **kwargs):\n        def custom_print(string, *margs, **mkwarg):\n            self.print(string, *args, *margs, **kwargs, **mkwarg)\n        return custom_print\n\n    @classmethod\n    def getpass(self, *args, **kwargs):\n        return self.print(*args, **kwargs, prompt=True, password=True)\n\n    @classmethod\n    def input(self, *args, **kwargs):\n        return self.print(*args, **kwargs, prompt=True)\n\n    @classmethod\n    def print(self, string='', persist=False, replace=False, time=False, \n            progress=None, frmt=None, inquiry=False, force=False, prompt=False,\n            password=False):\n        if self.silent and not force:\n            return\n        string = str(string)\n        rows, cols = os.popen('stty size', 'r').read().split()\n        rows = int(rows)\n        cols = int(cols)\n        print(('\\033[F'+' '*cols)*self.persist_rows, end='\\r')\n        if time:\n            string = self.time(string)\n        if progress is not None:\n            string = self.progress(string, progress)\n        if self.logfile is not None and not persist:\n            self.logfile.write(string + '\\n')\n            self.logfile.flush()\n        if frmt is not None:\n            if type(frmt) is list:\n                string = self.format(string, *frmt)\n            elif type(frmt) is str:\n                string = self.format(string, frmt)\n        if persist:\n            if not replace and self.persist_rows:\n                self.persist_str += '\\n' + string\n            else:\n                self.persist_str = string\n            self.persist_rows = self.render_rows(self.persist_str)\n        elif prompt:\n            if password:\n                return getpass(string)\n            else:\n                return input(string)\n        elif inquiry:\n            response = input(string).lower()\n            return response == 'y' or response == 'yes'\n        else:\n            print(string)\n        if self.persist_rows:\n            print(self.persist_str)\n")
    __stickytape_write_module('knowledge/util/__init__.py', '')
    __stickytape_write_module('knowledge/struct/transaction.py', "\nclass Transaction:\n\n    def __init__(self):\n        self.translations = []\n    \n\n    def load_translation(self, filepath, reverse=False):\n        '''load a concept code translation for convert codes directly\n\n        Note that translations need to loaded in the order that they\n        are intended to be applied to the transaction codes.    '''\n        \n        pass")
    __stickytape_write_module('knowledge/struct/dimension.py', '\nimport logging\n\nclass Dimension:\n    pass')
    __stickytape_write_module('knowledge/util/config.py', '\nimport json\nimport re\nimport os\n\nimport logging as log\n\nfrom knowledge.util.filesys import FilesysUtil\n\n\nclass ConfigUtil:\n    \'\'\'static class used to verify config to specs for runner\n\n    also includes some useful tools for handling config parameters\n    \'\'\'\n    types = {\'str\': str, \'int\': int, \'bool\': bool, \'dict\': dict,\n        \'list\': list, \'float\': float, \'null\': None}\n\n\n    @classmethod\n    def load_config(self, filepath):\n        \'load a config file; catches file and JSON errors\'\n        try:\n            with open(filepath) as handle:\n                return json.load(handle)\n        except FileNotFoundError as err:\n            log.error(f\'Config file {filepath} does not exist; \'\n                \'terminating model run.\')\n            raise err\n        except json.JSONDecodeError as err:\n            log.error(f\'Config file {filepath} is not valid json; \'\n                \'terminating model run.\')\n            raise err\n        \n\n    @classmethod\n    def load_specs(self, filepath):\n        \'load a config file; catches file and JSON errors\'\n        try:\n            with open(filepath) as handle:\n                return json.load(handle)\n        except FileNotFoundError as err:\n            log.error(f\'Specs file "{filepath}" does not exist; \'\n                \'terminating model run.\')\n            raise err\n        except json.JSONDecodeError as err:\n            log.error(f\'Specs file "{filepath}" is not valid json; \'\n                \'terminating model run.\')\n            raise err\n\n\n    @classmethod\n    def verify_config(self, specs, config, name=\'\'):\n        \'\'\'recursively validates a a configuration dict againast a \n        specifications dict\n\n        Parameters\n        ----------\n        specs: dict\n            The dict representing a JSON specifications file.\n\n        config: dict\n            The dict representing a JSON configurations file; will be checked\n            against the specifications file.\n\n        name: str\n            The name of the specification property; used in error handling. This\n            is set recursively; the intial name is an empty string.\n\n        Returns\n        -------\n        config: dict\n            A new dict representing the JSON configuration file with any missing\n            default values added onto it.\n        \'\'\'\n\n        for attr in config.keys():\n            if attr not in specs.keys():\n                if type(attr) is int:\n                    path = f\'{name}[{attr}]\' if name != \'\' else attr\n                else:\n                    path = f\'{name}.{attr}\' if name != \'\' else attr\n                log.warning(f\'Config parameter "{path}" is not in model \'\n                    \'specifications; it will not impact model run.\')\n\n        for attr, spec in specs.items():\n            param = config[attr] if attr in config else None\n            if type(attr) is int:\n                path = f\'{name}[{attr}]\' if name != \'\' else attr\n            else:\n                path = f\'{name}.{attr}\' if name != \'\' else attr\n\n            if type(spec) is not dict:\n                config[attr] = spec\n            elif \'type\' in spec:\n                if param is None:\n                    if \'required\' not in spec or not spec[\'required\']:\n                        config[attr] = spec[\'default\'] if \'default\' in spec else None\n                        continue\n                    else:\n                        log.error(f\'Parameter "{attr}" is required; see specs file.\')\n                        raise ValueError\n\n                if spec[\'type\'] == \'dict\':\n                    if type(param) == dict:\n                        if \'min\' in spec and spec[\'min\'] > len(param):\n                            log.error(f\'Parameter "{path}" expected to have at least \'\n                                f\'{spec["min"]} elements but only found {len(param)}.\')\n                            raise ValueError\n                        if \'max\' in spec and spec[\'max\'] < len(param):\n                            log.error(f\'Parameter "{path}" expected to have at most \'\n                                f\'{spec["max"]} elements but found {len(param)}.\')\n                            raise ValueError\n                        if \'options\' in spec and not all(k in spec[\'options\'] \n                                for k in param.keys()):\n                            valid = \'","\'.join(spec[\'options\'])\n                            invalid = \'","\'.join(k for k in param.keys() if \n                                k not in spec[\'options\'])\n                            log.error(f\'Parameter "{path}" expected to only \'\n                                f\'"{valid}" for keys but found "{invalid}".\')\n                            raise ValueError\n                        if \'struct\' in spec:\n                            spec = {k: spec[\'struct\'] for k in param.keys()}\n                            config[attr] = self.verify_config(spec, param, name=path)\n                    else:\n                        log.error(f\'Parameter "{path}" expected to be of type \'\n                            f\'"dict" but found "{type(param).__name__}".\')\n                        raise TypeError\n                elif spec[\'type\'] == \'list\':\n                    if type(param) == list:\n                        if \'min\' in spec and spec[\'min\'] > len(param):\n                            log.error(f\'Parameter "{path}" expected to have at least \'\n                                f\'{spec["min"]} elements but only found {len(param)}.\')\n                            raise ValueError\n                        if \'max\' in spec and spec[\'max\'] < len(param):\n                            log.error(f\'Parameter "{path}" expected to have at most \'\n                                f\'{spec["max"]} elements but found {len(param)}.\')\n                            raise ValueError\n                        if \'struct\' in spec:\n                            spec = {k: spec[\'struct\'] for k in range(len(param))}\n                            config[attr] = self.verify_config(spec,\n                                dict(enumerate(param)), name=path).values()\n                    else:\n                        log.error(f\'Parameter "{path}" expected to be of type \'\n                            f\'"list" but found "{type(param).__name__}".\')\n                        raise TypeError\n                else:\n                    self.verify_param(path, spec, param)\n            else:\n                config[attr] = self.verify_config(spec, param, name=path)\n        \n        return config\n\n    \n    @classmethod\n    def verify_param(self, name, spec, param):\n        \'\'\'validates a single config parameter against a specification\n\n        Parameters\n        ----------\n        name: str\n            Name of the attribute being checked; used in error handling.\n\n        spec: dict\n            A dict containing the attributes of the parameter.\n\n        param: int/float/str/bool\n            The config parameter being validated.\n        \'\'\'\n        types = [self.types[t] for t in spec[\'type\'].split(\',\')]\n\n        if type(param) not in types:\n            types = \'", "\'.join(spec[\'type\'].split(\',\'))\n            log.error(f\'Parameter "{name}" expected to be of type "\'\n                f\'{types}" but found "{type(param).__name__}".\')\n            raise TypeError\n\n        if (\'options\' in spec and len(spec[\'options\']) \n                and param not in spec[\'options\']):\n            options = \', \'.join([f\'"{str(opt)}"\' for opt in spec[\'options\']])\n            log.error(f\'Parameter "{name}" expected to be {options} \'\n                f\'but found "{param}".\')\n            raise ValueError\n\n        if \'exceptions\' in spec and param in spec[\'exceptions\']:\n            log.error(f\'Parameter "{name}" cannot be "{param}".\')\n            raise ValueError\n\n        if \'min\' in spec and param < spec[\'min\']:\n            log.error(f\'Parameter "{name}" expected to be greater than \'\n                f\'{spec["min"]} but found {name}.\')\n            raise ValueError\n\n        if \'max\' in spec and param > spec[\'max\']:\n            log.error(f\'Parameter "{name}" expected to be less than \'\n                f\'{spec["max"]} but found {param}.\')\n            raise ValueError\n\n        if \'regex\' in spec and not re.search(spec[\'regex\'], param):\n            log.error(f\'Parameter "{name}" expected to match pattern \'\n                f\'{spec["regex"]} but found "{param}".\')\n            raise ValueError\n\n        if \'file\' in spec:\n            if spec[\'file\'] == \'exists\' and not FilesysUtil.file_exists(param):\n                log.error(f\'Parameter "{name}" expected to be an existing file \'\n                    \'but file could not be found.\')\n                raise ValueError\n            elif spec[\'file\'] == \'readable\' and not FilesysUtil.file_readable(param):\n                log.error(f\'Parameter "{name}" expected to be an readable file \'\n                    \'but file could not be read.\')\n                raise ValueError\n            elif spec[\'file\'] == \'writable\' and not FilesysUtil.file_writable(param):\n                log.error(f\'Parameter "{name}" expected to be an writable file \'\n                    \'but file could not be written to.\')\n                raise ValueError\n')
    __stickytape_write_module('knowledge/model/__init__.py', '')
    __stickytape_write_module('knowledge/model/association/__init__.py', '')
    __stickytape_write_module('knowledge/model/association/basic/__init__.py', '')
    
    import sys
    import logging
    
    from argparse import ArgumentParser
    from getpass import getpass
    
    from knowledge.model.association.basic.model import BasicAssociationModel
    from knowledge.util.filesys import FilesysUtil
    
    # needed for pickling/unpickling large objects
    
    sys.setrecursionlimit(10000)
    
    # command line argument parsing
    
    parser = ArgumentParser(prog='association model runner',
        description='run an association building model')
    parser.add_argument('--config', type=str, dest='config', default=None,
        help=('specify a config file location; default is "config.json" in '
            'this this module\'s build directory'))
    parser.add_argument('--specs', type=str, dest='specs', default=None,
        help=('specify a specs file location; default is "specs.json" in '
            'this this module\'s build directory'))
    parser.add_argument('--log', type=str, dest='log', default=None,
        help='specify a log file location; by default the log will not be saved')
    args = parser.parse_args()
    
    # configure logging
    
    handlers = []
    handlers.append(logging.StreamHandler())
    frmt = '%(asctime)s %(levelname)s %(filename)s:%(lineno)s %(message)s'
    if args.log is not None:
        handlers.append(logging.FileHandler(args.log, 'w'))
    logging.basicConfig(
        format=frmt,
        level=logging.INFO,
        handlers=handlers)
    logging.info('Running association model module.')
    
    # check for config and specs files
    
    try:
        if args.config is None:
            args.config = FilesysUtil.package_resource(
                'knowledge.model.association.basic', 'config.json')
        if args.specs is None:
            args.specs = FilesysUtil.package_resource(
                'knowledge.model.association.basic', 'specs.json')
        if args.config is None or args.specs is None:
            raise FileNotFoundError
    except Exception:
        logging.exception('Default config/specs file(s) not found; fix packaging '
            ' or, if not running as a package, specify a config and specs with '
            '"--config" and "--specs" CLI arguments.')
        exit()
        
    # validate config file against specs file
    
    logging.info('Validating configuration with specifications.')
    config = BasicAssociationModel.configure(args.config, args.specs)
    
    # reconfigure logging (if applicable)
    
    if config['run']['log'] is not None and args.log is None:
        handlers.append(logging.FileHandler(config['run']['log'], 'w'))
        logging.basicConfig(
            format=frmt,
            level=getattr(logging, config['run']['verbosity']),
            handlers=handlers)
    
    # run model
    
    try:
        model = BasicAssociationModel(config)
        model.run()
    except Exception:
        logging.exception('Fatal error while running association model.')
        exit()
    
    logging.info('Assoication model run complete.')
    