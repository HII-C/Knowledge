#!/usr/bin/env python


import contextlib as __stickytape_contextlib

@__stickytape_contextlib.contextmanager
def __stickytape_temporary_dir():
    import tempfile
    import shutil
    dir_path = tempfile.mkdtemp()
    try:
        yield dir_path
    finally:
        shutil.rmtree(dir_path)

with __stickytape_temporary_dir() as __stickytape_working_dir:
    def __stickytape_write_module(path, contents):
        import os, os.path

        def make_package(path):
            parts = path.split("/")
            partial_path = __stickytape_working_dir
            for part in parts:
                partial_path = os.path.join(partial_path, part)
                if not os.path.exists(partial_path):
                    os.mkdir(partial_path)
                    open(os.path.join(partial_path, "__init__.py"), "w").write("\n")

        make_package(os.path.dirname(path))

        full_path = os.path.join(__stickytape_working_dir, path)
        with open(full_path, "w") as module_file:
            module_file.write(contents)

    import sys as __stickytape_sys
    __stickytape_sys.path.insert(0, __stickytape_working_dir)

    __stickytape_write_module('knowledge/model/association/model.py', '\nimport pickle\nimport logging as log\n\nfrom csv import writer\nfrom itertools import combinations\nfrom multiprocessing import Pool, Manager, Value\nfrom ctypes import c_uint64\n\nfrom knowledge.struct.fpgrowth import Fpgrowth\nfrom knowledge.util.config import ConfigUtil\nfrom knowledge.util.filesys import FilesysUtil\n\ntry:\n    from knowledge.struct.population import Population\n    mysql = True\nexcept ImportError:\n    mysql = False\n\n\n# globals for multiprocessing\ncount = None\nn = None\n\n\ndef chunks(lst, n):\n    for i in range(0, len(lst), n):\n        yield lst[i:i + n]\n\n\ndef write_associations(queue, csv, cols):\n    \'\'\'asynchronous queued association csv write\n\n    Parameters\n    ----------\n    queue: multiprocessing.Queue\n        File write queue as provided by the multiprocessing manager.\n\n    csv: str\n        String for the filepath to write associations.\n\n    cols: list[str]\n        List of column names for csv output.\n    \'\'\'\n    csvfile = open(csv, \'w\', newline=\'\')\n    csvwriter = writer(csvfile, delimiter=\',\', quotechar=\'"\')\n    csvwriter.writerow(cols)\n\n    request = queue.get()\n    while request != \'kill\':\n        csvwriter.writerows(request)\n        csvfile.flush()\n        request = queue.get()\n    csvfile.close()\n\n\ndef find_associations(queue, patterns, keys, min_support, min_confidence):\n    \'\'\'find associations in pattern chucnk and add to write queue\n\n    Parameters\n    ----------\n    queue: multiprocessing.Queue\n        File write queue as provided by the multiprocessing manager.\n\n    patterns: dict{frozenset[str]: float}\n        Dictionary of frequent patterns where a frozenset of items map\n        to their corresponding support.\n\n    keys: list[frozenset[str]]\n        List of frozensets that are keys to the patterns dictionary;\n        these patterns will be analyzed for association finding.\n\n    min_support: float\n\n    min_confidence: float\n\n    \'\'\'\n    inf = float(\'inf\')\n    metrics = {\n        \'support\':    lambda sAC, sA, sC: sAC,\n        \'confidence\': lambda sAC, sA, sC: sAC/sA,\n        \'lift\':       lambda sAC, sA, sC: sAC/sA/sC,\n        \'leverage\':   lambda sAC, sA, sC: sAC-sA*sC,\n        \'rpf\':        lambda sAC, sA, sC: sAC*sAC/sA,\n        \'conviction\': lambda sAC, sA, sC: (1-sC)/(1-sAC/sA) \\\n            if sAC != sA else inf   }\n    \n    associations = []\n    local_count = 0\n    for pattern in keys:\n        sAC = patterns[pattern]\n        for idx in range(len(pattern)-1,0,-1):\n            for subset in combinations(pattern, r=idx):\n                antecedent = frozenset(subset)\n                consequent = pattern - antecedent\n\n                sA = patterns[antecedent]\n                sC = patterns[consequent]\n\n                score = (metrics[\'support\'](sAC, sA, sC) >= min_support and\n                    metrics[\'confidence\'](sAC, sA, sC) >= min_confidence)\n\n                if score:\n                    if count.value >= n.value:\n                        log.info(f\'Found association {count.value}.\')\n                        with n.get_lock():\n                            n.value <<= 1\n                    \n                    if local_count >= 100000:\n                        queue.put(associations)\n                        associations = []\n                        local_count = 0 \n                        \n                    associations.append((\n                        \',\'.join(sorted(antecedent)),\n                        \',\'.join(sorted(consequent)),\n                        metrics[\'support\'](sAC, sA, sC),\n                        metrics[\'confidence\'](sAC, sA, sC),\n                        metrics[\'lift\'](sAC, sA, sC),\n                        metrics[\'leverage\'](sAC, sA, sC),\n                        metrics[\'conviction\'](sAC, sA, sC) if sAC != sA else None,\n                        metrics[\'rpf\'](sAC, sA, sC)))\n\n                    local_count += 1\n                    with count.get_lock():\n                        count.value += 1\n\n    queue.put(associations)\n\n\nclass AssociationModel:\n    def __init__(self, database=None):\n        if mysql and database is not None:\n            self.population = Population(database)\n            self.database = self.population.database\n        self.fpgrowth = None\n\n\n    @staticmethod\n    def mysql():\n        return mysql\n\n    \n    @staticmethod\n    def validate_config(configpath, specspath):\n        \'validates a configuration file for the association model\'\n        config = ConfigUtil.load_config(configpath)\n        specs = ConfigUtil.load_specs(specspath)\n        config = ConfigUtil.verify_config(specs, config)\n\n        tree = config[\'tree\']\n        association = config[\'associations\']\n        run = config[\'run\']\n\n        warn = False\n\n        if tree[\'load\'] and tree[\'save\']:\n            log.error(\'Does not make sense to both load and save tree.\')\n            warn = True\n        if tree[\'load\']:\n            if tree[\'pickle\'] is None:\n                log.error(\'Pickled tree path must be given to \'\n                    \'load pickled tree.\')\n                raise FileNotFoundError\n            if not FilesysUtil.file_readable(tree[\'pickle\']):\n                log.error(\'Pickled tree path must be a path to \'\n                    \'a readable file.\')\n                raise FileNotFoundError\n        if tree[\'save\']:\n            if tree[\'pickle\'] is None:\n                log.error(\'Pickled tree path must be given to \'\n                    \'save pickled tree.\')\n                raise FileNotFoundError\n            if not FilesysUtil.file_writable(tree[\'pickle\']):\n                log.error(\'Pickled tree path must be a \'\n                    \'path to a writable file.\')\n                raise FileNotFoundError\n\n        if association[\'load\'] and association[\'save\']:\n            log.error(\'Does not make sense to both load and save associations.\')\n            warn = True\n        if association[\'load\'] and not association[\'mysql\']:\n            log.error(\'Does not make sense to load associations without \'\n                \'saving them to mysql database.\')\n            warn = True\n        if association[\'load\']:\n            if association[\'csv\'] is None:\n                log.error(\'A csv path must be given to \'\n                    \'save associations as csv.\')\n            if not FilesysUtil.file_readable(association[\'csv\']):\n                log.error(\'Association csv path must be a \'\n                    \'path to a readable file.\')\n                raise FileNotFoundError\n        if association[\'save\'] :\n            if association[\'csv\'] is None:\n                log.error(\'A csv path must be given to \'\n                    \'save associations as csv.\')\n                raise FileNotFoundError\n            if not FilesysUtil.file_writable(association[\'csv\']):\n                log.error(\'Association csv path must be a \'\n                    \'path to a writable file.\')\n                raise FileNotFoundError\n\n        if not mysql:\n            if not tree[\'load\']:\n                log.error(\'No mysql detected; cannot generate population \'\n                    \'without mysql environment.\')\n                raise RuntimeError\n            if association[\'mysql\']:\n                log.error(\'No mysql detected; cannot save associations \'\n                    \'without mysql environment.\')\n                log.debug(\'See "association.mysql" config parameter.\')\n                raise RuntimeError\n\n        if run[\'goal\'] == \'tree\' and not tree[\'save\']:\n            log.warning(\'Goal is to generate tree but nothing is ever saved.\')\n            warn = True\n        if run[\'goal\'] == \'associations\' and not association[\'mysql\'] \\\n                and not association[\'save\']:\n            log.warning(\'Goal is to generate associations but nothing is ever saved.\')\n            warn = True\n\n        if warn:\n            log.warning(\'Continue given config validation warnings? [Y/n]\')\n            if input().lower() not in (\'y\', \'yes\'):\n                    log.error(\'User chose to terminate process.\')\n                    raise RuntimeError\n\n        return config\n\n\n    def run(self, config):\n        \'\'\'runs the association model with specified configuration\n\n        Parameters\n        ----------\n        config: dict\n            A dictionary representation of the JSON config file.\n            See documentation for specifications of the config file.\n        \'\'\'\n\n        force = config[\'run\'][\'force\']\n        goal = config[\'run\'][\'goal\']\n\n        log.info(\'Preallocating output tables/files.\')\n        if not force and config[\'tree\'][\'save\']:\n            save = config[\'tree\'][\'pickle\']\n            if FilesysUtil.file_exists(save):\n                log.warning(f\'Patterns tree pickle file "{save}" already \'\n                    \'exists. Delete and continue? [Y/n]\')\n                if input().lower() not in (\'y\', \'yes\'):\n                    log.error(\'User chose to terminate process.\')\n                    raise RuntimeError\n        if not force and config[\'associations\'][\'save\']:\n            save = config[\'associations\'][\'csv\']\n            if FilesysUtil.file_exists(save):\n                log.warning(f\'Association output file "{save}" already \'\n                    \'exists. Delete and continue? [Y/n]\')\n                if input().lower() not in (\'y\', \'yes\'):\n                    log.error(\'User chose to terminate process.\')\n                    raise RuntimeError\n        if mysql:\n            if config[\'associations\'][\'mysql\']:\n                self.create_tables(force)\n            if not config[\'tree\'][\'load\']:\n                self.population.create_tables(force)\n\n        if config[\'tree\'][\'load\']:\n            load = config[\'tree\'][\'pickle\']\n\n            log.info(f\'Loading pickled patterns tree from {load}.\')\n            self.fpgrowth = pickle.load(open(load, \'rb\'))\n            support = self.fpgrowth.support\n            popsize = self.fpgrowth.tree.root.count\n            items = [(key, val/popsize) for key, val in support.items()]\n\n            trans = self.fpgrowth.tree.root.count\n            events = sum(n.count for item in self.fpgrowth.tree.nodes.values()\n                for n in item) - trans\n            nodes = sum(len(item) for item in self.fpgrowth.tree.nodes.values())\n            log.info(f\'Tree complete with {len(items)} items, {trans} encounters, \'\n                f\'{events} events, and {nodes} nodes.\')\n        \n        else:\n            size = config[\'population\'][\'size\']\n\n            log.info(f\'Generating sample population of size {size}.\')\n            self.generate_population(**config[\'population\'])\n\n            log.info(\'First data scan; calculating support for population items.\')\n            support = self.calculate_support(**config[\'items\'])\n\n            log.info(\'Second data scan; building frequent patterns tree.\')\n            self.fpgrowth = Fpgrowth(support)\n            self.build_tree(config[\'items\'][\'source\'], config[\'run\'][\'bin\'])\n\n            items = self.population.items\n            trans = self.fpgrowth.tree.root.count\n            events = sum(n.count for item in self.fpgrowth.tree.nodes.values()\n                for n in item) - trans\n            nodes = sum(len(item) for item in self.fpgrowth.tree.nodes.values())\n            log.info(f\'Tree complete with {items} items, {trans} encounters, \'\n                f\'{events} events, and {nodes} nodes.\')\n\n        if config[\'tree\'][\'save\']:\n            save = config[\'tree\'][\'pickle\']\n            log.info(f\'Saving pickled patterns tree to {save}.\')\n            pickle.dump(self.fpgrowth, open(save, \'wb\'))\n\n        if goal == \'tree\':\n            log.info(\'Model achieved goal to build tree.\')\n            return\n            \n        log.info(\'Beginning reading frequent patterns from tree.\')\n        patterns = self.find_patterns(**config[\'patterns\'], \n            cores=config[\'run\'][\'cores\'])\n        del self.fpgrowth\n\n        if goal == \'patterns\':\n            log.info(\'Model achieved goal to find patterns.\')\n            return\n\n        log.info(\'Analyzing patterns for significant associations.\')\n        self.find_associations(patterns, **config[\'associations\'],\n            cores=config[\'run\'][\'cores\'])\n        del patterns\n\n        if config[\'run\'][\'index\']:\n            log.info(\'Creating indexes on all new tables.\')\n            for table in self.database.tables.keys():\n                self.database.create_all_idxs(table)\n\n        if goal == \'associations\':\n            log.info(\'Model achieved goal to find associations.\')\n            return\n\n        \n    def generate_population(self, source, size, rand=True, seed=None):\n        self.population.generate_population(source, size=size, rand=rand, seed=seed)\n        if size > self.population.encounters:\n            log.warning(f\'Requested population size of {size} but only found \'\n                f\' {self.population.encounters} encounters.\')\n\n\n    def create_tables(self, force=False):\n        if not force:\n            exists = self.database.table_exists(*list(self.database.tables.keys()))\n            tables = \'", "\'.join(exists)\n            if len(exists):\n                log.warning(f\'Table{"s" if len(tables) > 1 else ""} \'\n                    f\'"{tables}" already exist in database \'\n                    f\'"{self.database.db}". Drop and continue? [Y/n] \')\n                if input().lower() not in (\'y\', \'yes\'):\n                    log.error(\'User chose to terminate process.\')\n                    raise RuntimeError\n        for table in self.database.tables.keys():\n            self.database.create_table(table)\n        \n\n    def calculate_support(self, source, min_support, max_support):\n        \'calculate support for itemset; constrain itemset by support bounds\'\n        \n        self.population.generate_items(source, min_support, max_support)\n        items = self.population.fetch_items()\n\n        support = {key: val for key, val in items}\n\n        log.info(f\'Found {self.population.items} items in target population \'\n            f\'on support interval [{min_support}, {max_support}].\')\n\n        return support\n\n        \n    def build_tree(self, source, bin_size=100000):\n        \'\'\'iteratively build frequent patterns tree\n\n        Parameters\n        ----------\n        source: list[str]\n\n        bin_size: int\n        \'\'\'\n\n        for offset in range(0, self.population.encounters, bin_size):\n            lmt = min(bin_size, self.population.encounters)\n\n            log.info(f\'Fetching events for {lmt} encounters.\')\n            events = self.population.fetch_events(source, offset, bin_size)\n            log.info(f\'Retrieved {len(events)} events; inserting them\'\n                \' into the fpgrowth tree.\')\n            \n            encounters = []\n            items = set()\n            hadmid = 0\n            for event in events:\n                if event[0] != hadmid:\n                    hadmid = event[0]\n                    if len(items):\n                        encounters.append(items)\n                    items = set()\n                items.add(event[1])\n            del events\n\n            matrix, items = Fpgrowth.matrix(encounters)\n            del encounters\n\n            self.fpgrowth.build_tree(matrix, items)\n            del matrix\n            del items\n\n\n    def find_patterns(self, min_support, max_support, max_size, cores=None):\n        \'\'\'read patterns of of frequent patterns tree\n        \'\'\'\n        encounters = self.fpgrowth.tree.root.count\n        min_support = int(min_support * encounters)\n        max_support = int(max_support * encounters)\n\n        patterns_array = self.fpgrowth.find_patterns(self.fpgrowth.tree, min_support, \n            max_support, max_size, cores)\n        \n        patterns_dict = {frozenset(p[1]): p[0] / encounters for p in patterns_array}\n\n        return patterns_dict\n\n\n    def find_associations(self, patterns, min_support=0, min_confidence=0,\n            csv=None, save=False, cores=None):\n        \'\'\'find associations from frequent patterns dictionary\n        \'\'\'\n        if csv is None:\n            csvfile = FilesysUtil.create_tempfile(suffix=\'csv\', delete=False)\n            csv = csvfile.name\n            csvfile.close()\n            temp = True\n        else:\n            temp = False\n\n        log.info(f\'Balancing patterns into tasks for {cores} cores.\')\n\n        global count, n\n        count = Value(c_uint64)\n        count.value = 0\n        n = Value(c_uint64)\n        n.value = 1\n\n        manager = Manager()\n        queue = manager.Queue(maxsize=10)\n        pool = Pool(processes=cores)\n\n        cols = (\'antecedent\', \'consequent\', \'support\', \'confidence\', \'lift\',\n            \'leverage\', \'conviction\', \'rpf\')\n        pool.apply_async(write_associations, (queue, csv, cols))\n\n        chunksize = len(patterns) // (cores * 4)\n        tasks = [(queue, patterns, keys, min_support, min_confidence) for keys in \n            chunks(list(patterns.keys()), chunksize)]\n\n        log.info(f\'Finding associations and writing them to "{csv}".\')\n        pool.starmap(find_associations, tasks)\n        queue.put(\'kill\')\n        pool.close()\n        pool.join()\n\n        if count.value != n.value >> 1:\n            log.info(f\'Found association {count.value}.\')\n\n        if mysql:\n            log.info(\'Loading associations into mysql database.\')\n            self.load_associations(csv)\n\n        if temp:\n            log.info(f\'Deleting temporary associations file "{csv}".\')\n            FilesysUtil.delete_file(csv)\n\n        \n    def load_associations(self, csv):\n        query = \'ALTER TABLE associations DISABLE KEYS\'\n        self.database.cursor.execute(query)\n        self.database.connection.commit()\n        query = f\'\'\'\n            LOAD DATA LOCAL INFILE {csv} INTO TABLE associations\n            FIELDS \n                TERMINATED BY \\\',\\\'\n                ESCAPED BY \\\'\\\\\\\' \n                OPTIONALLY ENCLOSED BY \\\'"\\\'\n            LINES \n                TERMINATED BY \\\'\\n\\\'\n            IGNORE 1 LINES (\n                @antecedent,\n                @consequent,\n                @support,\n                @confidence,\n                @lift,\n                @leverage,\n                @conviction,\n                @rpf    )\n            SET\n                antecedent = @antecedent,\n                consequent = @consequent,\n                support = @support,\n                confidence = @confidence,\n                lift = @lift,\n                leverage = @leverage,\n                conviction = @conviction,\n                rpf = @rpf  \'\'\'\n        self.database.cursor.execute(query)\n        self.database.connection.commit()\n        query = \'ALTER TABLE associations ENABLE KEYS\'\n        self.database.cursor.execute(query)\n        self.database.connection.commit()\n        query = \'OPTIMIZE TABLE associations\'\n        self.database.cursor.execute(query)\n        self.database.connection.commit()\n        ')
    __stickytape_write_module('knowledge/struct/fpgrowth.py', "\nimport pandas as pd\nimport numpy as np\nimport logging as log\n\nfrom collections import defaultdict\nfrom itertools import combinations\nfrom multiprocessing import Pool, Value\nfrom ctypes import c_uint64\n\n\n# globals for multiprocessing\ncount = None\nn = None\n\n\ndef find_patterns(tree, min_support, max_support, max_size):\n    '''pattern finding function for a single thread\n    '''\n    generator = Fpgrowth.generate_patterns(tree, min_support, \n        max_support, max_size)\n    patterns = []\n    for pattern in generator:\n        patterns.append(pattern)\n        if count.value >= n.value:\n            with n.get_lock():\n                n.value <<= 1\n            log.info(f'Found pattern {count.value}.')\n        with count.get_lock():\n            count.value += 1\n    return patterns\n\n\nclass Fpgrowth:\n    '''data structure and utilities for running fpgrowth algorithm\n\n    Parameters\n    ----------\n    support: dict{str: int}\n        A dictionary mapping the names of items to their support;\n        not the support here is an integer between zero and the \n        number of transactions, not a float between 0 ad 1.\n    '''\n\n    def __init__(self, support):\n        self.tree = Tree()\n        self.support = support\n    \n\n    @staticmethod\n    def matrix(transactions):\n        '''converts list of transactions into a sparse matrix\n\n        Parameters\n        ----------\n        transactions: list[list]\n            A list of transactions, which are each a list of items.\n\n        Returns\n        -------\n        matrix: numpy.ndarray\n            A numpy array representing the bool matrix of the transactions.\n\n        items: tuple\n            A list containing the items names for the matrix, which \n            correspond to the columns of the matrix.\n        '''\n\n        unique = set()\n        for trans in transactions:\n            for item in trans:\n                unique.add(item)\n\n        matrix = np.zeros((len(transactions),len(unique)), dtype=bool)\n        items = {key: val for val, key in enumerate(unique)}\n\n        for idx, trans in enumerate(transactions):\n            for item in trans:\n                matrix[idx, items[item]] = True\n\n        return matrix, tuple(unique)\n\n\n    def build_tree(self, matrix, items, silent=False):\n        '''appends transactions to tree\n\n        Parameters\n        ----------\n        matrix: numpy.ndarray\n            A numpy array representing the bool matrix of the transactions.\n\n        items: tuple\n            A list containing the items names for the matrix, which \n            correspond to the columns of the matrix. This set of items must be \n            a subset of the support attribute.\n\n        silent: bool = False\n            If true, this process will not print algorithm progress messages;\n            default is false.\n        '''\n\n        for i in range(len(matrix)):\n            itemset = [item for cond, item in zip(matrix[i], items) if cond \n                and item in self.support]\n            itemset.sort(key=self.support.get, reverse=True)\n            self.tree.insert_itemset(itemset)\n\n\n    def find_patterns(self, tree, min_support=0, max_support=1, \n            max_size=0, cores=None):\n        '''finds patterns from tree using multiprocessing\n\n        Parameters\n        ----------\n        tree: Tree\n            The tree to read patterns from\n\n        min_support: int\n            The minimum support of a pattern for it to be included in the\n            list of frequent patterns.\n\n        max_support: int\n            \n        max_size: int\n\n        cores: int/None\n            Number of cores to utilize; default is None, which will auto detect\n            the number of cores available to use.\n\n        Returns\n        -------\n        patterns: list[list[float, list[str]]]\n            A list of patterns, where each pattern is a list containing\n            the pattern support value and list of items in the pattern.\n        '''\n        log.info(f'Balancing tree into tasks for {cores} cores.')\n        items = tree.nodes.keys()\n        subtrees = []\n        patterns = []\n\n        for item in items:\n            subtree = tree.conditional_tree(item, min_support, max_support)\n            subtrees.append((subtree, min_support, max_support, max_size))\n        subtrees.sort(key=lambda tree: tree[0].root.count_descendents(), reverse=True)\n        \n        global count\n        count = Value(c_uint64)\n        count.value = 0\n        global n\n        n = Value(c_uint64)\n        n.value = 1\n\n        log.info(f'Finding patterns tree root branch.')\n\n        for item in items:\n            support = sum([node.count for node in tree.nodes[item]])\n            patterns.append((support, (item,)))\n            if count.value >= n.value:\n                log.info(f'Found pattern {count.value}.')\n                n.value <<= 1\n            count.value += 1\n\n        log.info(f'Now finding remaining patterns on {cores} cores.')\n\n        pool = Pool(processes=cores)\n        for result in pool.starmap(find_patterns, subtrees):\n            patterns.extend(result)\n        if count.value != n.value >> 1:\n            log.info(f'Found pattern {count.value}.')\n        pool.close()\n        pool.join()\n        \n        return patterns\n\n        \n    @classmethod\n    def generate_patterns(self, tree, min_support=0, max_support=1, max_size=0):\n        '''recursively generates frequent patterns off of tree\n        \n        Parameters\n        ----------\n        tree: Tree\n            The tree to read patterns from; this function will continue to\n            recursively itself with subtrees until the tree is a path or empty.\n\n        min_support: int\n            The minimum support of a pattern for it to be included in the\n            list of frequent patterns.\n\n        max_support: int\n\n        max_size: int\n\n\n        Yields\n        ------\n        pattern: tuple(float, list[str])\n        '''\n\n        items = tree.nodes.keys()\n        if tree.is_path():\n            size_remain = len(items) + 1\n            if max_size:\n                size_remain = max_size - len(tree.items) + 1\n            for i in range(1, size_remain):\n                for itemset in combinations(items, i):\n                    support = min([tree.nodes[i][0].count for i in itemset])\n                    yield support, tree.items + list(itemset)\n        elif max_size == 0  or max_size > len(tree.items):            \n            for item in items:\n                support = sum([node.count for node in tree.nodes[item]])\n                yield support, tree.items + [item]\n                \n            for item in items:\n                subtree = tree.conditional_tree(item, min_support, max_support)\n                for support, itemset in self.generate_patterns(subtree, \n                        min_support, max_support, max_size):\n                    yield support, itemset\n\n\nclass Tree:\n    '''a tree structure with variable length, unordered children\n\n    Parameters\n    ----------\n    root: str/int/float = None\n        The value of the root node; default is None.\n    '''\n\n    def __init__(self, root=None):\n        self.root = Node(root)\n        self.nodes = defaultdict(list)\n        self.items = []\n    \n\n    def insert_itemset(self, itemset, count=1):\n        node = self.root\n        node.count += count\n\n        idx = 0\n        for item in itemset:\n            if item in node.children:\n                node = node.children[item]\n                node.count += count\n                idx += 1\n            else:\n                break\n\n        for item in itemset[idx:]:\n            child = Node(item, count=count, parent=node)\n            node.children[item] = child\n            self.nodes[item].append(child)\n            node = child\n            \n\n    def conditional_tree(self, cond, min_support, max_support):\n        branches = []\n        count = defaultdict(int)\n        for node in self.nodes[cond]:\n            branch = node.itempath()\n            branches.append(branch)\n            for item in branch:\n                count[item] += node.count\n                \n        items = [item for item in count if count[item] >= min_support\n            and count[item] <= max_support]\n        items.sort(key=count.get)\n        rank = {item: i for i, item in enumerate(items)}\n\n        tree = Tree()\n        tree.items = self.items + [cond]\n        for idx, branch in enumerate(branches):\n            branch = sorted([node for node in branch if node in rank],\n                key=rank.get, reverse=True)\n            tree.insert_itemset(branch, self.nodes[cond][idx].count)\n\n        return tree\n\n\n    def is_path(self):\n        node = self.root\n        while len(node.children) == 1:\n            node = list(node.children.values())[0]\n        return len(node.children) == 0\n\n\nclass Node:\n    '''an element in a tree structure\n\n    Parameters\n    ----------\n    item: str/int/float\n        A value describing the name of the item the node is associated with.\n    \n    count: int = 1\n        What value to initialize the count of the node on; default is 1.\n\n    parent: Node = None\n        The parent node of the node being constructed; default is None type.\n    '''\n\n    def __init__(self, item, count=1, parent=None):\n        self.item = item\n        self.count = count\n        self.children = {}\n        self.parent = parent\n\n\n    def count_descendents(self, n=None):\n        if n is not None and n <= 0:\n            return 1\n        else:\n            return sum(node.count_descendents() for node in self.children.values()) + 1\n\n\n    def itempath(self):\n        path = []\n        parent = self.parent\n        while parent.item is not None:\n            path.append(parent.item)\n            parent = parent.parent\n        path.reverse()\n        return path")
    __stickytape_write_module('knowledge/__init__.py', '')
    __stickytape_write_module('knowledge/struct/__init__.py', '')
    __stickytape_write_module('knowledge/util/config.py', '\nimport json\nimport re\nimport os\n\nimport logging as log\n\nfrom knowledge.util.filesys import FilesysUtil\n\n\nclass ConfigUtil:\n    \'\'\'static class used to verify config to specs for runner\n\n    also includes some useful tools for handling config parameters\n    \'\'\'\n    types = {\'str\': str, \'int\': int, \'bool\': bool, \'dict\': dict,\n        \'list\': list, \'float\': float, \'null\': None}\n\n\n    @classmethod\n    def load_config(self, filepath):\n        \'load a config file; catches file and JSON errors\'\n        try:\n            with open(filepath) as handle:\n                return json.load(handle)\n        except FileNotFoundError as err:\n            log.error(f\'Config file {filepath} does not exist; \'\n                \'terminating model run.\')\n            raise err\n        except json.JSONDecodeError as err:\n            log.error(f\'Config file {filepath} is not valid json; \'\n                \'terminating model run.\')\n            raise err\n        \n\n    @classmethod\n    def load_specs(self, filepath):\n        \'load a config file; catches file and JSON errors\'\n        try:\n            with open(filepath) as handle:\n                return json.load(handle)\n        except FileNotFoundError as err:\n            log.error(f\'Specs file "{filepath}" does not exist; \'\n                \'terminating model run.\')\n            raise err\n        except json.JSONDecodeError as err:\n            log.error(f\'Specs file "{filepath}" is not valid json; \'\n                \'terminating model run.\')\n            raise err\n\n\n    @classmethod\n    def verify_config(self, specs, config, name=\'\'):\n        \'\'\'recursively validates a a configuration dict againast a \n        specifications dict\n\n        Parameters\n        ----------\n        specs: dict\n            The dict representing a JSON specifications file.\n\n        config: dict\n            The dict representing a JSON configurations file; will be checked\n            against the specifications file.\n\n        name: str\n            The name of the specification property; used in error handling. This\n            is set recursively; the intial name is an empty string.\n\n        Returns\n        -------\n        config: dict\n            A new dict representing the JSON configuration file with any missing\n            default values added onto it.\n        \'\'\'\n\n        for attr in config.keys():\n            if attr not in specs.keys():\n                if type(attr) is int:\n                    path = f\'{name}[{attr}]\' if name != \'\' else attr\n                else:\n                    path = f\'{name}.{attr}\' if name != \'\' else attr\n                log.warning(f\'Config parameter "{path}" is not in model \'\n                    \'specifications; it will not impact model run.\')\n\n        for attr, spec in specs.items():\n            param = config[attr] if attr in config else None\n            if type(attr) is int:\n                path = f\'{name}[{attr}]\' if name != \'\' else attr\n            else:\n                path = f\'{name}.{attr}\' if name != \'\' else attr\n\n            if type(spec) is not dict:\n                config[attr] = spec\n            elif \'type\' in spec:\n                if param is None:\n                    if \'required\' not in spec or not spec[\'required\']:\n                        config[attr] = spec[\'default\'] if \'default\' in spec else None\n                        continue\n                    else:\n                        log.error(f\'Parameter "{attr}" is required; see specs file.\')\n                        raise ValueError\n\n                if spec[\'type\'] == \'dict\':\n                    if type(param) == dict:\n                        if \'min\' in spec and spec[\'min\'] > len(param):\n                            log.error(f\'Parameter "{path}" expected to have at least \'\n                                f\'{spec["min"]} elements but only found {len(param)}.\')\n                            raise ValueError\n                        if \'max\' in spec and spec[\'max\'] < len(param):\n                            log.error(f\'Parameter "{path}" expected to have at most \'\n                                f\'{spec["max"]} elements but found {len(param)}.\')\n                            raise ValueError\n                        if \'options\' in spec and not all(k in spec[\'options\'] \n                                for k in param.keys()):\n                            valid = \'","\'.join(spec[\'options\'])\n                            invalid = \'","\'.join(k for k in param.keys() if \n                                k not in spec[\'options\'])\n                            log.error(f\'Parameter "{path}" expected to only \'\n                                f\'"{valid}" for keys but found "{invalid}".\')\n                            raise ValueError\n                        if \'struct\' in spec:\n                            spec = {k: spec[\'struct\'] for k in param.keys()}\n                            config[attr] = self.verify_config(spec, param, name=path)\n                    else:\n                        log.error(f\'Parameter "{path}" expected to be of type \'\n                            f\'"dict" but found "{type(param).__name__}".\')\n                        raise TypeError\n                elif spec[\'type\'] == \'list\':\n                    if type(param) == list:\n                        if \'min\' in spec and spec[\'min\'] > len(param):\n                            log.error(f\'Parameter "{path}" expected to have at least \'\n                                f\'{spec["min"]} elements but only found {len(param)}.\')\n                            raise ValueError\n                        if \'max\' in spec and spec[\'max\'] < len(param):\n                            log.error(f\'Parameter "{path}" expected to have at most \'\n                                f\'{spec["max"]} elements but found {len(param)}.\')\n                            raise ValueError\n                        if \'struct\' in spec:\n                            spec = {k: spec[\'struct\'] for k in range(len(param))}\n                            config[attr] = self.verify_config(spec,\n                                dict(enumerate(param)), name=path).values()\n                    else:\n                        log.error(f\'Parameter "{path}" expected to be of type \'\n                            f\'"list" but found "{type(param).__name__}".\')\n                        raise TypeError\n                else:\n                    self.verify_param(path, spec, param)\n            else:\n                config[attr] = self.verify_config(spec, param, name=path)\n        \n        return config\n\n    \n    @classmethod\n    def verify_param(self, name, spec, param):\n        \'\'\'validates a single config parameter against a specification\n\n        Parameters\n        ----------\n        name: str\n            Name of the attribute being checked; used in error handling.\n\n        spec: dict\n            A dict containing the attributes of the parameter.\n\n        param: int/float/str/bool\n            The config parameter being validated.\n        \'\'\'\n        types = [self.types[t] for t in spec[\'type\'].split(\',\')]\n\n        if type(param) not in types:\n            types = \'", "\'.join(spec[\'type\'].split(\',\'))\n            log.error(f\'Parameter "{name}" expected to be of type "\'\n                f\'{types}" but found "{type(param).__name__}".\')\n            raise TypeError\n\n        if (\'options\' in spec and len(spec[\'options\']) \n                and param not in spec[\'options\']):\n            options = \', \'.join([f\'"{str(opt)}"\' for opt in spec[\'options\']])\n            log.error(f\'Parameter "{name}" expected to be {options} \'\n                f\'but found "{param}".\')\n            raise ValueError\n\n        if \'exceptions\' in spec and param in spec[\'exceptions\']:\n            log.error(f\'Parameter "{name}" cannot be "{param}".\')\n            raise ValueError\n\n        if \'min\' in spec and param < spec[\'min\']:\n            log.error(f\'Parameter "{name}" expected to be greater than \'\n                f\'{spec["min"]} but found {name}.\')\n            raise ValueError\n\n        if \'max\' in spec and param > spec[\'max\']:\n            log.error(f\'Parameter "{name}" expected to be less than \'\n                f\'{spec["max"]} but found {param}.\')\n            raise ValueError\n\n        if \'regex\' in spec and not re.search(spec[\'regex\'], param):\n            log.error(f\'Parameter "{name}" expected to match pattern \'\n                f\'{spec["regex"]} but found "{param}".\')\n            raise ValueError\n\n        if \'file\' in spec:\n            if spec[\'file\'] == \'exists\' and not FilesysUtil.file_exists(param):\n                log.error(f\'Parameter "{name}" expected to be an existing file \'\n                    \'but file could not be found.\')\n                raise ValueError\n            elif spec[\'file\'] == \'readable\' and not FilesysUtil.file_readable(param):\n                log.error(f\'Parameter "{name}" expected to be an readable file \'\n                    \'but file could not be read.\')\n                raise ValueError\n            elif spec[\'file\'] == \'writable\' and not FilesysUtil.file_writable(param):\n                log.error(f\'Parameter "{name}" expected to be an writable file \'\n                    \'but file could not be written to.\')\n                raise ValueError\n')
    __stickytape_write_module('knowledge/util/filesys.py', "\nimport os\nimport tempfile\nimport subprocess\nimport logging as log\n\nfrom knowledge.util.print import PrintUtil\n\nclass FilesysUtil:\n    @classmethod\n    def file_readable(self, filepath):\n        'check that file can be read'\n        return os.access(filepath, os.R_OK)\n\n\n    @classmethod\n    def file_exists(self, filepath):\n        'check that file exists'\n        return os.path.exists(filepath)\n    \n\n    @classmethod\n    def file_writable(self, filepath):\n        'check that file can be written to'\n        if self.file_exists(filepath):\n            if os.path.isfile(filepath):\n                return os.access(filepath, os.W_OK)\n            else:\n                return False \n        \n        pdir = os.path.dirname(filepath)\n        if not pdir: \n            pdir = '.'\n        return os.access(pdir, os.W_OK)\n\n\n    @classmethod\n    def create_tempfile(self, suffix=None, delete=True):\n        return tempfile.NamedTemporaryFile(suffix=suffix, delete=delete)\n\n    \n    @classmethod\n    def delete_file(self, filepath):\n        os.remove(filepath)\n\n\n    @classmethod\n    def format_xml(self, source, target=None):\n        'format an xml file'\n        if target is None:\n            targetfile = self.create_tempfile(suffix='xml', delete=False)\n            target = targetfile.name\n            targetfile.close()\n            result = subprocess.run(f'xmllint --format {source} > {target}', shell=True)\n            if result:\n                log.error(f'Failed to format XML on {target}.')\n            else:\n                subprocess.run(('mv', target, source), shell=False)\n        else:\n            result = subprocess.run(f'xmllint --format {source} > {target}', shell=True)\n            if result:\n                log.error(f'Failed to format XML from {source} to {target}.')\n        \n        return bool(result)\n\n\n    @classmethod\n    def decompress(self, source, target=None, keep=False):\n        'decompress a gz file'\n        keep = '--keep' if keep else ''\n        if target is None:\n            targetfile = self.create_tempfile(\n                suffix=source.split('.')[-2], delete=False)\n            target = targetfile.name\n            targetfile.close()\n        subprocess.run(f'gunzip --stdout {keep} {source} > {target}', shell=True)\n\n        return target\n\n\n    @classmethod\n    def compress(self, source, target=None, keep=False):\n        pass")
    __stickytape_write_module('knowledge/util/print.py', "\nimport math\nimport os\nimport re\nimport time\nimport subprocess\n\nfrom datetime import datetime\nfrom getpass import getpass\n\nclass PrintUtil:\n    persist_str = ''\n    persist_rows = 0\n    logfile = None\n    silent = False\n    FRMTS = {\n        'bold': 1,\n        'faint': 2,\n        'italic': 3,\n        'underline': 4,\n        'strikethrough': 9\n    }\n\n    @classmethod\n    def silence(self):\n        self.silent = True\n\n    @classmethod\n    def unsilence(self):\n        self.silence = False\n\n    @classmethod\n    def log(self, filename):\n        self.logfile = open(filename, 'w')\n\n    @classmethod\n    def render_width(self, string):\n        # https://en.wikipedia.org/wiki/ANSI_escape_code\n        return len(re.sub('\\\\x1b\\[[0-9]*m', '', string))\n\n    @classmethod\n    def render_rows(self, string):\n        _, cols = map(int, subprocess.run(('stty', 'size'), \n            stdout=subprocess.PIPE).stdout.decode().replace('\\n', '').split())\n        return sum(self.render_width(line) // cols + 1 for line \n            in string.split('\\n'))\n\n    @classmethod\n    def table(self, tbl, align='l', pad=1, border=False, wrap=False, hrule=None):\n        if align in ('r', 'l'):\n            aligns = [align] * len(tbl[0])\n        elif type(align) in (list, tuple) and all([a in ('r', 'l') for a in align]):\n            aligns = align \n        else:\n            return ''\n        if type(pad) is int:\n            pads = [pad] * len(tbl[0])\n        elif type(pad) in (list, tuple) and all([type(p) is int for p in pad]):\n            pads = pad\n        else:\n            return ''\n        tbl = [[str(cell) for cell in row] for row in tbl]\n        widths = [max([self.render_width(cell) for cell in col]) \n            for col in list(map(list, zip(*tbl)))]\n        if border:\n            if hrule is None:\n                hrules = [0]*(len(tbl))\n            elif (type(hrule) in (list, tuple) \n                    and all([type(h) is int for h in hrules])):\n                hrules = [1 if i in hrule else 0 for i in range(len(tbl)-1)] + [0]\n            top = '+' + '+'.join('-'*(w+p*2) for w, p in zip(widths, pads)) + '+'\n            return (top + '\\n' +\n                '\\n'.join('|' + '|'.join([' '*p + cell.ljust(w) + ' '*p \n                if a == 'l' else ' '*p + cell.rjust(w) + ' '*p\n                for cell, w, a, p in zip(row, widths, aligns, pads)]) + \n                (f'|\\n{top}' if hrule else '|')\n                for row, hrule in zip(tbl, hrules)) + '\\n' + top)\n        else:\n            return '\\n'.join(''.join(cell.ljust(w) + ' '*p\n                if a == 'l' else cell.rjust(w) + ' '*p\n                for cell, w, a, p in zip(row, widths, aligns, pads)) \n                for row in tbl)\n    \n    @classmethod\n    def time(self, string):\n        date = datetime.now()\n        return ('[' + date.strftime('%H:%M:%S:') + \n            str(date.microsecond // 1000).zfill(3) +\n            '] ' + string)\n\n    @classmethod\n    def format(self, string, *frmts):\n        # https://en.wikipedia.org/wiki/ANSI_escape_code\n        codes = tuple(self.FRMTS[frmt] for frmt in frmts if frmt in self.FRMTS)\n        return ('\\x1b[%sm'*len(codes) % codes) + string + '\\x1b[0m'        \n    \n    @classmethod\n    def progress(self, string, prog):\n        prog = min(1, max(0, prog))\n        perc = 100 * prog\n        return ( string + ' [' + \n                '=' * int(perc // 5) + \n                '_' * int(20 - perc // 5) + \n                '] ' + str(round(perc, 1)) + '%')\n\n    @classmethod\n    def clear(self, rows=None):\n        if rows is None:\n            rows, cols = os.popen('stty size', 'r').read().split()\n            rows = int(rows)\n            cols = int(cols)\n        print('\\n'*(rows-1) + '\\033[F'*rows, end='\\r')\n\n    @classmethod\n    def delete(self, rows):\n        pass\n\n    @classmethod\n    def push(self):\n        persist = self.persist_str\n        self.print('', persist=True, replace=True)\n        self.print(persist)\n\n    @classmethod\n    def printer(self, *args, **kwargs):\n        def custom_print(string, *margs, **mkwarg):\n            self.print(string, *args, *margs, **kwargs, **mkwarg)\n        return custom_print\n\n    @classmethod\n    def getpass(self, *args, **kwargs):\n        return self.print(*args, **kwargs, prompt=True, password=True)\n\n    @classmethod\n    def input(self, *args, **kwargs):\n        return self.print(*args, **kwargs, prompt=True)\n\n    @classmethod\n    def print(self, string='', persist=False, replace=False, time=False, \n            progress=None, frmt=None, inquiry=False, force=False, prompt=False,\n            password=False):\n        if self.silent and not force:\n            return\n        string = str(string)\n        rows, cols = os.popen('stty size', 'r').read().split()\n        rows = int(rows)\n        cols = int(cols)\n        print(('\\033[F'+' '*cols)*self.persist_rows, end='\\r')\n        if time:\n            string = self.time(string)\n        if progress is not None:\n            string = self.progress(string, progress)\n        if self.logfile is not None and not persist:\n            self.logfile.write(string + '\\n')\n            self.logfile.flush()\n        if frmt is not None:\n            if type(frmt) is list:\n                string = self.format(string, *frmt)\n            elif type(frmt) is str:\n                string = self.format(string, frmt)\n        if persist:\n            if not replace and self.persist_rows:\n                self.persist_str += '\\n' + string\n            else:\n                self.persist_str = string\n            self.persist_rows = self.render_rows(self.persist_str)\n        elif prompt:\n            if password:\n                return getpass(string)\n            else:\n                return input(string)\n        elif inquiry:\n            response = input(string).lower()\n            return response == 'y' or response == 'yes'\n        else:\n            print(string)\n        if self.persist_rows:\n            print(self.persist_str)\n")
    __stickytape_write_module('knowledge/util/__init__.py', '')
    __stickytape_write_module('knowledge/struct/population.py', '\nimport logging as log\n\nfrom knowledge.util.database import DatabaseUtil\nfrom knowledge.util.error import UserExitError\n\n\nclass Population:\n    \'\'\'population managing class\n\n    must call `generate_population` to create population before\n    fetching patients, encounters or events\n    \'\'\'\n    def __init__(self, database):\n        self.database = DatabaseUtil(params=database)\n\n        self.mimic = \'mimiciiiv14\'\n        self.synthea = \'synthea\'\n        self.umls = \'umls\'\n        self.rxnorm = \'rxnorm\'\n\n        self.population = False\n        self.source = None\n        self.patients = None\n        self.encounters = None\n        self.items = None\n\n\n    def create_tables(self, force=False):\n        tables = (\'patients\', \'encounters\', \'items\')\n        if not force:\n            exists = self.database.table_exists(*tables)\n            tables = \'", "\'.join(exists)\n            if len(exists):\n                log.warning(f\'Table{"s" if len(tables) > 1 else ""} \'\n                    f\'"{tables}" already exist in database \'\n                    f\'"{self.database.db}". Drop and continue? [Y/n] \')\n                if input().lower() not in (\'y\', \'yes\'):\n                    log.error(\'User chose to terminate process.\')\n                    raise RuntimeError\n        for table in self.database.tables.keys():\n            self.database.create_table(table)\n\n\n    def delete_tables(self, tables=[]):\n        \'\'\'delete/reset current population\n        \'\'\'\n        query = \'DROP TABLE IF EXISTS %s\'\n        if len(tables) == 0:\n            tables = (\'patients\', \'encounters\', \'items\')\n        for tbl in tables:\n            self.database.cursor.execute(query % tbl)\n            self.database.connection.commit()\n\n        self.population = False\n        self.source = None\n        self.patients = None\n        self.encounters = None\n\n\n    def generate_population(self, source, size=None, rand=False, seed=None):\n        \'\'\'generate a population tables\n\n        Parameters\n        ----------\n        source: str\n            The source database to generate population from, options are\n            "mimic" and "synthea".\n        size: int\n            A maximum number of patients in population.\n        rand: bool\n            Specify whether or not to randomly sort the population; default\n            is false.\n        seed: int\n            Specify a seed for the random sorting, which can make populations\n            reproducable; default is null.\n        \'\'\'\n        if source == \'synthea\':\n            db = self.synthea\n            tbl = \'patients\'\n            col = \'id\'\n        elif source == \'mimic\':\n            db = self.mimic\n            tbl = \'PATIENTS\'\n            col = \'SUBJECT_ID\'\n        else:\n            raise ValueError(\'Population expected source to be "synthea" or \'\n                f\'"mimic" but got "{source}".\')\n        \n        self.source = source\n        self.population = True\n\n        log.info(\'Generating population patients.\')\n        query = f\'\'\'\n            CREATE TABLE patients\n            SELECT\n                {col} AS patient\n            FROM {db}.{tbl}\n            {f"ORDER BY RAND({seed if seed else \'\'})" if rand else ""}\n            {f"LIMIT {size}" if size is not None else ""} \'\'\'\n        self.database.cursor.execute(query)\n        self.database.connection.commit()\n\n        if source == \'synthea\':\n            db = self.synthea\n            tbl = \'encounters\'\n            col1 = \'id\'\n            col2 = \'patient\'\n        elif source == \'mimic\':\n            db = self.mimic\n            tbl = \'ADMISSIONS\'\n            col1 = \'HADM_ID\'\n            col2 = \'SUBJECT_ID\'\n\n        log.info(\'Generating population encounters.\', time=True)\n        query = f\'\'\'\n            CREATE TABLE encounters\n            SELECT\n                enc.{col2} AS patient,\n                enc.{col1} AS encounter\n            FROM {db}.{tbl} AS enc\n            INNER JOIN patients\n            ON enc.{col2} = patients.patient\n            ORDER BY patient \'\'\'\n        self.database.cursor.execute(query)\n        self.database.connection.commit()\n\n        query = f\'\'\'\n            CREATE INDEX encounter\n            USING HASH\n            ON encounters(encounter) \'\'\'\n        self.database.cursor.execute(query)\n        self.database.connection.commit()\n\n        query = f\'\'\'\n            SELECT COUNT(*)\n            FROM patients\n            UNION\n            SELECT COUNT(*)\n            FROM encounters \'\'\'\n        self.database.cursor.execute(query)\n        self.patients, self.encounters = [int(row[0])\n            for row in self.database.cursor.fetchall()]\n\n    \n    def generate_items(self, source, min_support=0, max_support=1):\n        \'\'\'generate items table with support\n\n        Parameters\n        ----------\n\n        \'\'\'\n        source = set(source)\n        if not source.issubset((\'observations\', \'treatments\', \'conditions\')):\n            raise ValueError()\n        if not self.population:\n            raise RuntimeError(\'Must generate a population before generating \'\n                \'items from it.\')\n\n        if self.source == \'synthea\':\n            subquery = {}\n            subquery[\'observations\'] = f\'\'\'\n                SELECT\n                    CONCAT("O-", obs.code)  AS item,\n                    COUNT(*) AS support\n                FROM {self.synthea}.observations AS obs\n                INNER JOIN encounters\n                USING(encounter) \n                GROUP BY item \'\'\'\n            subquery[\'conditions\'] = f\'\'\'\n                SELECT\n                    CONCAT("C-", cnd.code) AS item,\n                    COUNT(*)\n                FROM {self.synthea}.conditions AS cnd\n                INNER JOIN encounters\n                USING(encounter)\n                GROUP BY item \'\'\'\n            subquery[\'treatments\'] = f\'\'\'\n                SELECT\n                    CONCAT("C-", trt.code) AS item,\n                    COUNT(*) AS support\n                FROM {self.synthea}.medications AS trt\n                INNER JOIN encounters\n                USING(encounter)\n                GROUP BY item \'\'\'\n        elif self.source == \'mimic\':\n            subquery = {}\n            subquery[\'observations\'] = f\'\'\'\n                SELECT\n                    CONCAT("O-", itm.LOINC_CODE)  AS item,\n                    COUNT(*) AS support\n                FROM {self.mimic}.LABEVENTS AS obs\n                INNER JOIN encounters AS enc\n                ON enc.encounter = obs.HADM_ID\n                INNER JOIN {self.mimic}.D_LABITEMS AS itm\n                ON obs.ITEMID = itm.ITEMID\n                WHERE itm.LOINC_CODE IS NOT NULL\n                GROUP BY item \'\'\'\n            subquery[\'conditions\'] = f\'\'\'\n                SELECT\n                    CONCAT("C-", cnd.ICD9_CODE) AS item,\n                    COUNT(*)\n                FROM {self.mimic}.DIAGNOSES_ICD AS cnd\n                INNER JOIN encounters AS enc\n                ON enc.encounter = cnd.HADM_ID\n                WHERE cnd.ICD9_CODE IS NOT NULL\n                GROUP BY item \'\'\'\n            subquery[\'treatments\'] = f\'\'\'\n                SELECT\n                    CONCAT("T-", trt.NDC) AS item,\n                    COUNT(*)\n                FROM {self.mimic}.PRESCRIPTIONS AS trt\n                INNER JOIN encounters AS enc\n                ON enc.encounter = trt.HADM_ID\n                WHERE trt.NDC IS NOT NULL\n                GROUP BY item \'\'\'\n        else:\n            raise ValueError(\'Population expected source to be "synthea" or \'\n                f\'"mimic" but got "{source}".\')\n\n        subquery = [val for key, val in subquery.items() if key in source]\n        query = \'\'\'\n            CREATE TABLE items (\n                item VARCHAR(255),\n                support INT UNSIGNED )\n            SELECT *\n            FROM ( \'\'\'\n        query += \'\\nUNION\\n\'.join(subquery)\n        query += f\'\'\'\n            ) AS subquery\n            WHERE support >= {min_support * self.encounters}\n            AND support <= {max_support * self.encounters} \'\'\'\n        self.database.cursor.execute(query)\n        self.database.connection.commit()\n\n        query = f\'\'\'\n            SELECT COUNT(*)\n            FROM items \'\'\'\n        self.database.cursor.execute(query)\n        self.items = int(self.database.cursor.fetchall()[0][0])\n\n    \n    def fetch_patients(self, size=None):\n        \'\'\'fetch patients from population\n\n        Parameters\n        ----------\n        size: int\n            A maximum number of rows to return.\n\n        Returns\n        -------\n        result: list[list[str]]\n            A list of lists containing the patient codes.\n        \'\'\'\n        if self.population:\n            query = f\'\'\'\n                SELECT patient\n                FROM patients\n                {f"LIMIT {size}" if size is not None else ""} \'\'\'\n        else:\n            raise RuntimeError(\'Must generate a population before fetching \'\n                \'patients from it.\')\n\n        self.database.cursor.execute(query)\n        return self.database.cursor.fetchall()\n\n\n    def fetch_encounters(self, size=None):\n        \'\'\'fetch encounters from population\n\n        Parameters\n        ----------\n        size: int\n            A maximum number of rows to return.\n\n        Returns\n        -------\n        result: list[list[str]]\n            A list of lists containing the patient and encounter codes\n            respectively. The table is sorted by the encounter code.\n        \'\'\'\n        if self.population:\n            query = f\'\'\'\n                SELECT\n                    patient,\n                    encounter\n                FROM encounters\n                {f"LIMIT {size}" if size is not None else ""} \'\'\'\n        else:\n            raise RuntimeError(\'Must generate a population before fetching \'\n                \'encounters from it.\')\n\n        self.database.cursor.execute(query)\n        return self.database.cursor.fetchall()\n\n\n    def fetch_items(self):\n        \'\'\'fetch support from items\n        \'\'\'\n        if self.items is None:\n            raise RuntimeError(\'Must generate items before fetching \'\n                \'support from it.\')\n\n        query = f\'\'\'\n            SELECT\n                item,\n                support\n            FROM items \'\'\'\n        self.database.cursor.execute(query)\n        return self.database.cursor.fetchall()\n\n    \n    def fetch_events(self, source, offset=None, limit=None):\n        \'\'\'fetch events from population\n\n        Parameters\n        ----------\n        source: str\n            A list of strings containing the types of events to fetch.\n            Options are "observations", "treatments", and "conditions".\n            List can be any nonempty subset of these options.\n\n        offset: int\n\n        limit: int\n\n        Returns\n        -------\n        result: list[list[str]]\n            A list of lists containing the encounter and event codes\n            respectively. The table is sorted by the encounter code.\n        \'\'\'\n        source = set(source)\n        if not source.issubset((\'observations\', \'treatments\', \'conditions\')):\n            raise ValueError()\n        if not self.population:\n            raise RuntimeError(\'Must generate a population before fetching \'\n                \'events from it.\')\n\n        if self.source == \'synthea\':\n            subquery = {}\n            subquery[\'observations\'] = f\'\'\'\n                SELECT\n                    enc.encounter,\n                    CONCAT("O-", obs.code)\n                FROM {self.synthea}.observations AS obs\n                INNER JOIN (\n                    SELECT encounter\n                    FROM encounters\n                    LIMIT {offset}, {limit}\n                ) AS enc\n                USING(encounter) \'\'\'\n            subquery[\'conditions\'] = f\'\'\'\n                SELECT\n                    enc.encounter,\n                    CONCAT("C-", cnd.code)\n                FROM {self.synthea}.conditions AS cnd\n                INNER JOIN (\n                    SELECT encounter\n                    FROM encounters\n                    LIMIT {offset}, {limit}\n                ) AS enc\n                USING(encounter) \'\'\'\n            subquery[\'treatments\'] = f\'\'\'\n                SELECT\n                    enc.encounter,\n                    CONCAT("C-", trt.code)\n                FROM {self.synthea}.medications AS trt\n                INNER JOIN (\n                    SELECT encounter\n                    FROM encounters\n                    LIMIT {offset}, {limit}\n                ) AS enc\n                USING(encounter) \'\'\'\n        elif self.source == \'mimic\':\n            subquery = {}\n            subquery[\'observations\'] = f\'\'\'\n                SELECT\n                    enc.encounter,\n                    CONCAT("O-", itm.LOINC_CODE)\n                FROM {self.mimic}.LABEVENTS AS obs\n                INNER JOIN (\n                    SELECT encounter\n                    FROM encounters\n                    LIMIT {offset}, {limit}\n                ) AS enc\n                ON enc.encounter = obs.HADM_ID\n                INNER JOIN {self.mimic}.D_LABITEMS AS itm\n                USING(ITEMID)\'\'\'\n            subquery[\'conditions\'] = f\'\'\'\n                SELECT\n                    enc.encounter,\n                    CONCAT("C-", cnd.ICD9_CODE)\n                FROM {self.mimic}.DIAGNOSES_ICD AS cnd\n                INNER JOIN (\n                    SELECT encounter\n                    FROM encounters\n                    LIMIT {offset}, {limit}\n                ) AS enc\n                ON enc.encounter = cnd.HADM_ID \'\'\'\n            subquery[\'treatments\'] = f\'\'\'\n                SELECT\n                    enc.encounter,\n                    CONCAT("C-", trt.NDC)\n                FROM {self.mimic}.PRESCRIPTIONS AS trt\n                INNER JOIN (\n                    SELECT encounter\n                    FROM encounters\n                    LIMIT {offset}, {limit}\n                ) AS enc\n                ON enc.encounter = trt.HADM_ID \'\'\'\n        else:\n            raise ValueError(\'Population expected source to be "synthea" or \'\n                f\'"mimic" but got "{source}".\')\n\n        subquery = {key: val for key, val in subquery.items() \n            if key in source}\n        query = \'\\nUNION\\n\'.join(subquery.values())\n        query += \'\\nORDER BY encounter\'\n        self.database.cursor.execute(query)\n        return self.database.cursor.fetchall()')
    __stickytape_write_module('knowledge/util/database.py', '\nimport MySQLdb\nimport warnings\n\nimport logging as log\n\n# supress MySQL warnings\nwarnings.filterwarnings(\'ignore\', category=MySQLdb._exceptions.Warning)\n\nclass DatabaseUtil:\n    def __init__(self, params=None, database=None):\n        if type(database) is DatabaseUtil:\n            self = database\n        elif type(params) is dict:\n            keys = (\'user\', \'password\', \'db\', \'host\', \'unix_socket\')\n            login = {key: val for key, val in params.items() \n                if key in keys and val is not None}\n            self.user = params[\'user\']\n            self.host = params[\'host\']\n            self.db = params[\'db\']\n            self.tables = params[\'tables\'] if \'tables\' in params else {}\n            try:\n                self.connection = MySQLdb.connect(**login)\n                self.cursor = self.connection.cursor()\n            except MySQLdb._exceptions.OperationalError as err:\n                if err.args[0] == 1049:\n                    log.warning(f\'Database "{self.db}" does not exist. Create \'\n                        \'and continue? [Y/n] \')\n                    if input().lower() not in (\'y\', \'yes\'):\n                        del login[\'db\']\n                        connection = MySQLdb.connect(**login)\n                        cursor = connection.cursor()\n                        cursor.execute(f\'CREATE DATABASE {params["db"]}\')\n                        connection.commit()\n                        cursor.close()\n                        connection.close()\n                        login[\'db\'] = self.db\n                        self.connection = MySQLdb.connect(**login)\n                        self.cursor = self.connection.cursor()\n                    else:\n                        log.error(\'User chose to terminate process.\')\n                        raise RuntimeError\n                else:\n                    log.error(\'MySQL login/connection failure.\')\n                    raise err\n\n    def drop_table(self, table):\n        query = f\'DROP TABLE IF EXISTS {self.db}.{table}\'\n        self.cursor.execute(query)\n        self.connection.commit()\n\n    def table_exists(self, *tables):\n        conditions = f\'" or `Tables_in_{self.db}` like "\'.join(tables)\n        query = f\'\'\'\n            SHOW TABLES FROM {self.db}\n            WHERE `Tables_in_{self.db}` like "{conditions}"\'\'\'\n        self.cursor.execute(query)\n        return [row[0] for row in self.cursor.fetchall()]\n\n    def create_table(self, table):\n        table_data = self.tables[table]\n        sql_schema = (\', \').join(table_data[\'schema\'])\n        query = f\'DROP TABLE IF EXISTS {self.db}.{table}\'\n        self.cursor.execute(query)\n        self.connection.commit()\n        query = f\'CREATE TABLE {self.db}.{table} ({sql_schema})\'\n        self.cursor.execute(query)\n        self.connection.commit()\n\n    def create_index(self, table, name):\n        columns = (\', \').join(self.tables[table][\'indexes\'][name])\n        query = f\'\'\'\n            CREATE INDEX {name}\n            ON {self.db}.{table} ({columns})\'\'\'\n        self.cursor.execute(query)\n        self.connection.commit()\n\n    def create_primary_idx(self, table):\n        cols = \', \'.join(self.tables[table][\'primary_idx\'])\n        query = f\'\'\'\n            ALTER TABLE {self.db}.{table}\n            ADD PRIMARY KEY ({cols})\'\'\'\n        self.cursor.execute(query)\n        self.connection.commit()\n\n    def create_hash_idx(self, table, name):\n        cols = \', \'.join(self.tables[table][\'hash_idxs\'][name])\n        query = f\'\'\'\n            CREATE INDEX {name} USING HASH\n            ON {self.db}.{table} ({cols})\'\'\'\n        self.cursor.execute(query)\n        self.connection.commit()        \n\n    def create_btree_idx(self, table, name):\n        cols = \', \'.join(self.tables[table][\'btree_idxs\'][name])\n        query = f\'\'\'\n            CREATE INDEX {name}\n            ON {self.db}.{table} ({cols})\'\'\'\n        self.cursor.execute(query)\n        self.connection.commit()   \n    \n    def create_spatial_idx(self, table, name):\n        cols = \', \'.join(self.tables[table][\'spatial_idxs\'][name])\n        query = f\'\'\'\n            CREATE SPATIAL INDEX {name}\n            ON {self.db}.{table} ({cols})\'\'\'\n        self.cursor.execute(query)\n        self.connection.commit()\n\n    def create_fulltext_idx(self, table, name):\n        cols = \', \'.join(self.tables[table][\'fulltext_idxs\'][name])\n        query = f\'\'\'\n            CREATE FULLTEXT INDEX {name}\n            ON {self.db}.{table} ({cols})\'\'\'\n        self.cursor.execute(query)\n        self.connection.commit()       \n    \n    def create_all_idxs(self, table):\n        tbl_data = self.tables[table]\n        if \'primary_idx\' in tbl_data and tbl_data[\'primary_idx\'] is not None:\n            if len(tbl_data[\'primary_idx\']):\n                log.info(f\'Creating primary index on table "{table}".\')\n                self.create_primary_idx(table)\n        if \'spatial_idxs\' in tbl_data and tbl_data[\'spatial_idxs\'] is not None:\n            for idx in tbl_data[\'spatial_idxs\']:\n                log.info(f\'Creating spatial index "{idx}" on table "{table}".\')\n                self.create_spatial_idx(table, idx)\n        if \'hash_idxs\' in tbl_data and tbl_data[\'hash_idxs\'] is not None:\n            for idx in tbl_data[\'hash_idxs\']:\n                log.info(f\'Creating hash index "{idx}" on table "{table}".\')\n                self.create_hash_idx(table, idx)\n        if \'btree_idxs\' in tbl_data and tbl_data[\'btree_idxs\'] is not None:\n            for idx in tbl_data[\'btree_idxs\']:\n                log.info(f\'Creating btree index "{idx}" on table "{table}".\')\n                self.create_btree_idx(table, idx)\n        if \'fulltext_idxs\' in tbl_data and tbl_data[\'fulltext_idxs\'] is not None:\n            for idx in tbl_data[\'fulltext_idxs\']:\n                log.info(f\'Creating fulltext index "{idx}" on table "{table}".\')\n                self.create_fulltext_idx(table, idx)\n\n    def write_rows(self, data, table):\n        s_strs = \', \'.join([\'%s\'] * len(self.tables[table][\'schema\']))\n        query = f\'\'\' \n            INSERT INTO {self.db}.{table}\n            VALUES ({s_strs}) \'\'\'\n        self.cursor.executemany(query, data)\n        self.connection.commit()\n')
    __stickytape_write_module('knowledge/util/error.py', '\nclass ConfigError(Exception):\n    pass\n\nclass UserExitError(Exception):\n    pass')
    __stickytape_write_module('knowledge/model/__init__.py', '')
    __stickytape_write_module('knowledge/model/association/__init__.py', '')
    
    import sys
    import logging
    
    from pkgutil import get_data
    from argparse import ArgumentParser
    from getpass import getpass
    
    from knowledge.model.association.model import AssociationModel
    
    # needed for pickling/unpickling large trees
    sys.setrecursionlimit(10000)
    
    # command line argument parsing
    parser = ArgumentParser(prog='association model runner',
        description='run an association building model')
    parser.add_argument('--config', type=str, dest='config', default=None,
        help=('specify a config file location; default is "config.json" in '
            'this this module\'s build directory'))
    parser.add_argument('--specs', type=str, dest='specs', default=None,
        help=('specify a specs file location; default is "specs.json" in '
            'this this module\'s build directory'))
    parser.add_argument('--log', type=str, dest='log', default=None,
        help='specify a log file location; by default the log will not be saved')
    args = parser.parse_args()
    
    # configure logging
    hanlders = []
    hanlders.append(logging.StreamHandler())
    if args.log is not None:
        hanlders.append(logging.FileHandler(args.log, 'w'))
    logging.basicConfig(
        format='%(asctime)s %(levelname)s %(filename)s:%(lineno)s %(message)s',
        level=logging.INFO,
        handlers=hanlders)
    logging.info('Running association model module.')
    
    # check for config and specs files
    try:
        if args.config is None:
            args.config = get_data('knowledge', 'model/association/config.json')
        if args.specs is None:
            args.specs = get_data('knowledge', 'model/association/specs.json')
        if args.config is None or args.specs is None:
            raise FileNotFoundError
    except Exception as err:
        logging.error('Default config/specs file not found; fix packaging or, if'
            'not running as a package, specify a config and specs with '
            '"--config" and "--specs" CLI arguments.')
        raise err
        
    # validate config file against specs file
    logging.info('Validating configuration with specifications.')
    config = AssociationModel.validate_config(args.config, args.specs)
    
    # database credentials handling
    if AssociationModel.mysql():
        database = config['database']
        if database['user'] is None:
            database['user'] = input('SQL username for localhost: ')
        if database['user'] is None or database['password'] is None:
            logging.debug(f'SQL password for {database["user"]}@localhost: ')
            database['password'] = getpass('')
    else:
        logging.info('Model running without SQL environment.')
    
    # run model
    try:
        model = AssociationModel(database)
        model.run(config)
    except Exception:
        logging.exception('Fatal error while running association model.')
    
    logging.info('Assoication model run complete.')
    